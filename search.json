{"entries":[{"title":"404: Page not found","url":"/404.html","date":null,"categories":[],"body":" 404: Page not found Sorry, we've misplaced that URL or it's pointing to something that doesn't exist. Head back home to try finding it again. "},{"title":"About","url":"/about/","date":null,"categories":[],"body":" I am a Software Developer & Entrepreneur. I use Ruby and JavaScript as my tools of choice usually bulding applications in Rails and/or AngularJS. To find out more about the professional me check my LinkedIN profile. To find out more about the personal me check my FB profile Startups I am currently running two startups: One is focused on 'bootcamp' style of Software Developer training and the other is a Ruby and JavaScript consultancy firm aimed at working with Startups in Asia Pacific region. Dev Bootstrap Dev Bootstrap is a 3 month intensive Software Developer training program designed to help developers gain solid, real world experience developing actual applications and land a dream job! Rotati Rotati is a Ruby and JavaScript consultancy that works primarily with Startups to solve early stage problems, growth hacking experiments as well as custom web and mobile development. "},{"title":"Archive","url":"/archive/","date":null,"categories":[],"body":"Blog Posts 08 Jul 2014 » Network Data Visualization graph using SigmaJS 04 Jun 2014 » Using postgres hstore with Rails / Active Record 02 Apr 2014 » Rails View Template Inheritance 21 Mar 2014 » Testing Rails ActionMailer using RSpec Shared Examples 11 Mar 2014 » Legacy Rails app (well 2.3.x) with (almost) latest Rspec, Capybara & Factory Girl 04 Mar 2014 » Dissecting thesmallestrailsapp.com (Smallest Rails App: Newsflash) 31 May 2013 » Testing infinite scroll using RSpec and Capybara (without sleep or wait_until) 04 Apr 2013 » Create Nested Polymorphic Comments with Rails and Ancestry Gem 07 Jan 2013 » Dissecting thesmallestrailsapp.com (Smallest Rails App: line 4) 31 Dec 2012 » Dissecting thesmallestrailsapp.com (Smallest Rails App: line 3) 22 Oct 2012 » Setting up a captcha with reCAPTCHA service and the captcha gem 18 Sep 2012 » Using Railtie and Rails Engine in Gems 11 Sep 2012 » Mixing Ruby in Bash 22 Aug 2012 » Dissecting thesmallestrailsapp.com (Smallest Rails App) 25 Apr 2012 » Forking Ruby Processes or How to fork Ruby 30 Mar 2012 » Create reusable UI components and widgets using Backbone.js 16 Feb 2012 » Ruby Generators 23 Jan 2012 » Basic multiple image gallery upload HTML5 and Backbone application 19 Jan 2012 » Save an image file directly to S3 from a web browser using HTML5 and Backbone.js 18 Jan 2012 » Preview a local image using Backbone and HTML5 JavaScript 05 Jan 2012 » Ruby blocks, Procs and lambdas 22 Dec 2011 » Getting started with the HTML5 Geolocation API 10 Dec 2011 » Connecting a HTML5 application to a MongoDB instance via MongoLab REST API 12 Nov 2011 » Getting started with the HTML5 Audio Video API 08 Oct 2011 » Getting started with the HTML5 Canvas API 30 Sep 2011 » HTML5 Series: What makes HTML5 different? 25 Jul 2011 » Using Value Objects in ActiveRecord 13 Jun 2011 » Learn Rails Backwards 20 May 2011 » Migrate a PHP application to Rails 3 21 Apr 2011 » Why is Bundler in Rails 3 so slow? 15 Apr 2011 » Installing Rails gem fails with “undefined method `spec’ for nil:NilClass” error 05 Apr 2011 » Moving from Apache to Cherokee Web Server 22 Feb 2011 » How to get a semantic list of all spoken languages in the world 18 Feb 2011 » How to get a semantic list of all countries and locations in the world 10 Feb 2011 » Setting join table attribute :has_many :through association in Rails ActiveRecord 24 Jan 2011 » (Almost) Everthing an experienced programmer needs to know to get started with PHP 05 Jan 2011 » Using MongoDB as a logging database with a Rails 3 application 03 Jan 2011 » Install MongoDB and load dummy logging data using MongoMapper 24 Dec 2010 » So this is how Chrome will take over our desktops! 20 Dec 2010 » Install WordPress on Ubuntu 10.04 Slicehost (Part 2) 20 Dec 2010 » Clone a VirtualBox VDI using VBoxManage on the command line 15 Dec 2010 » A new JavaScript UI Framework is born: YoolkUI 14 Dec 2010 » Install WordPress on Ubuntu 10.04 Slicehost 27 Nov 2010 » command to ftp in linux bash script 15 Sep 2010 » Time to please Facebook not Google 15 Sep 2010 » Let the social networking diaspora begin! 08 Sep 2010 » What should the first blog post be about? 27 Jun 2010 » Using Google Analytics API in a Ruby on Rails project 14 May 2010 » Hudson setup to execute scripts with root privileges (safely) 14 Apr 2010 » How to subdomain redirect using Apache Virtual Host 19 Mar 2010 » Step by step guide to setting up a client server workstation on Ubuntu and VirtualBox 14 Mar 2010 » How to return content from an application running on a specific port in apache 29 Jan 2010 » Create a WCF Rest Application using .NET 3.5 11 Jan 2010 » The easy way to create a new Ruby Gem! 30 Dec 2009 » Remote desktop using a single tsclient terminal command 30 Dec 2009 » Compare Amazon S3 to EBS data read performance 22 Nov 2009 » Tagging in Mercurial 19 Oct 2009 » Transform a DataSet to JSON using Json.NET (Part 2) 17 Sep 2009 » Restart Ubuntu GUI without restarting Ubuntu 11 Sep 2009 » Transform a DataSet to JSON using Json.NET 06 Jul 2009 » Update ZoneEdit NameServers via a Restful API in Ruby 06 Jul 2009 » How to create an excel spreadsheet using Java (JExcel actually) 16 Jun 2009 » Setting up a scheduled task (cron job) to run a rake script in Ubuntu "},{"title":"Home","url":"/","date":null,"categories":[],"body":" Older Newer "},{"title":"Home","url":"/search/","date":null,"categories":[],"body":" $(function() { $('#search-query').lunrSearch({ indexUrl: '/search.json', // URL of the `search.json` index data for your site results: '#search-results', // jQuery selector for the search results container entries: '.entries', // jQuery selector for the element to contain the results list, must be a child of the results element above. template: '#search-results-template' // jQuery selector for the Mustache.js template }); }); {{#entries}} <article> <h3> {{#date}}<small><time datetime=\"{{pubdate}}\" pubdate>{{displaydate}}</time></small>{{/date}} <a href=\"{{url}}\">{{title}}</a> </h3> </article> {{/entries}} Search results "},{"title":"Setting up a scheduled task (cron job) to run a rake script in Ubuntu","url":"/2009/06/setting-up-a-scheduled-task-cron-job-to-run-a-rake-script-in-ubuntu/","date":"2009-06-16 00:00:00 +0700","categories":["programming"],"body":"To schedule a task in Ubuntu you need to understand cron and crontab. Both these concepts are easy to grasp. However, nothing is always as smooth as you would want! The issues I had to resolve are: Understanding the syntax of cron Adding a crontab for a user Creating an executable shell script which calls rake Logging the output For the syntax, I recommend checking out this Wikipedia article on cron . I will explain each of the above areas, starting with adding a crontab for a user. Adding a crontab for a user In order to edit a crontab for a user, you must type the following in the terminal: sudo crontab -e Note that I prefix the command with “sudo” which means it will modify the cron file for the user root. When I first tried this there was the following error: “no crontab for root”. I solved this problem with the following script which simply creates an empty file and associates that as a crontab for user root. touch rootcron sudo crontab -u root rootcron Now you will be able to edit the file using sudo crontab -e and it will work just fine. Creating an executable shell script which calls rake Let’s say you have a rake script in /var/www/someproject/rakefile. You can execute the rake script using the following script: #!/bin/sh date echo \"Executing rake\" cd /var/www/someproject/ rake You must make this script an executable before cron can start it up. Here’s how: sudo chmod +x runrake.sh You can now execute this every 24hrs at midnight (for example) by adding the following line to your cron. Note that the full path to the script is included and there is a “./” before the name of the script, which is how executable files can be run in the terminal. Next issue. Sometimes the cron job would run but not complete or just did not appear to run at all. How can I get more information about what is happening? Let’s check out logging! 0 0 * * * sudo /var/www/someproject/./runrake.sh Logging the output It turns out to be really straight forward to log the output of a cron job. Simply use the redirect output syntax to send the standard output to a file. Use single > to create a new file each time the cron is run or double >> to append to your log file. You can put the file anywhere you like, however, it is recommended to put inside the standard log folder /var/log. So now the line in the cron file looks as follows: 0 0 * * * sudo /var/www/someproject/./runrake.sh > /var/log/runrake.log "},{"title":"How to create an excel spreadsheet using Java (JExcel actually)","url":"/2009/07/how-to-create-an-excel-spreadsheet-using-java-jexcel-actually/","date":"2009-07-06 00:00:00 +0700","categories":["programming"],"body":"If you want to manipulate an Excel spreadsheet using .NET it is very easy to do. If your not convinced by that statement, I will show you how in a separate post. However, the purpose of this post is to create an Excel spreadsheet and populate it with values using something other than .NET. There may be many reasons why you would want to do this and mine was simply because I wanted to stick to using Linux (Ubuntu 9.10 Server). I gave up with trying it out in Mono – some basic ADO.NET code I had written just would not run! So I turned to Java! Someone on my team recommended I try JExcel and so I did. It turned out to be a simple excercise to spike. Here is the class: import java.io.File ; import java.io.IOException ; import java.io.OutputStream ; import jxl.Workbook ; import jxl.write.Label ; import jxl.write.WritableSheet ; import jxl.write.WritableWorkbook ; public class JExcelSample { public static void main ( String args []) { try { File f = new File ( \"JExcelSample.xls\" ); WritableWorkbook w = Workbook . createWorkbook ( f ); WritableSheet s = w . createSheet ( \"Demo\" , 0 ); s . addCell ( new Label ( 0 , 0 , \"Hello World\" )); w . write (); w . close (); } catch ( Exception e ) { System . err . println ( e ); } } } To compile and run the above Java program you will need to download and save jxl.jar to your /usr/lib directory and run the following statement in your terminal: javac - cp \"/usr/lib/jxl.jar\" JExcelSample . java java - cp \"/usr/lib/jxl.jar\" JExcelSample That’s it! After you have run the program you will see the JExcelSample.xls file and if you open it you will see that it contains one worksheet called Demo with one entry “Hello World”. Note that you are not required to have Excel installed on your machine to do any of the above. The code runs just fine without and the spreadsheet will open in OpenOffice.org Spreadsheet by default. "},{"title":"Update ZoneEdit NameServers via a Restful API in Ruby","url":"/2009/07/update-zoneedit-nameservers-via-a-restful-api-in-ruby/","date":"2009-07-06 00:00:00 +0700","categories":["programming","ruby server programming"],"body":"I recently had to programmatically alter entries in ZoneEdit . While there is some documentation, it is difficult to get started quickly. This is especially true if you want to use Ruby since there are no examples written in Ruby that I could find. So I wrote a simple class to do just what I need and here it is: 1 require 'net/http' 2 require 'net/https' 3 require 'uri' 4 5 class ZoneEdit 6 7 #Initialize with the ZoneEdit username and password credentials 8 def initialize ( username , password ) 9 @@username = username 10 @@password = password 11 end 12 13 #This method can be used to change a dns record in ZoneEdit 14 #Pass in the account username and zone you want to manage as well as the dnsfrom (sub-domain) forward address and 0 or 1 to shadow (cloak) or not 15 def web_forward ( user , zone , dnsfrom , forward , shadow ) 16 return send_command ( \"command=ChangeRecord&#038;user= #{ user } &#038;zone= #{ zone } &#038;type=WF&#038;dnsfrom= #{ dnsfrom } &#038;forward= #{ forward } &#038;shadow= #{ shadow } \" ) 17 end 18 19 private 20 def send_command ( command ) 21 puts \"https://www.zoneedit.com/auth/admin/command.html? #{ command } \" 22 @http = Net :: HTTP . new ( 'www.zoneedit.com' , 443 ) 23 @http . use_ssl = true 24 @http . start () { | http | 25 req = Net :: HTTP :: Get . new ( \"/auth/admin/command.html? #{ command } \" ) 26 req . basic_auth @@username , @@password 27 response = http . request ( req ) 28 print response . body 29 } 30 end 31 32 end It’s usage is simply as follows: 1 ze = ZoneEdit . new ( \"someuser\" , \"apassword\" ) 2 ze . web_forward \"someuser\" , \"somedomain.com\" , \"somesubdomain\" , \"http://anewdomain.com\" , \"1\" "},{"title":"Transform a DataSet to JSON using Json.NET","url":"/2009/09/transform-a-dataset-to-json-using-json-net/","date":"2009-09-11 00:00:00 +0700","categories":["programming"],"body":"It was my duty recently to convert some data from SQL Server 2005 into JSON. Initially, I started writing a bunch of UDF’s in SQL Server that built up a string representation of the data in JSON format. The problem with this method is that it is fiddly, slow and does not take into account JSON data types. So I turned to Json.NET by James Newton-King . What I liked about this library is it’s support for LINQ and XML. Here is a basic method for converting a DataSet that contains one table into JSON. Note the call to AsEnumerable() on the DataTable so that we can then use the LINQ support provided by Json.NET. private static string ConvertDataToJson () { JObject trans = new JObject ( new JProperty ( \"data\" , new JArray ( from r in ds . Tables [ 0 ]. AsEnumerable () select new JObject ( ConvertRowToJPropertyArray ( r ) )))); return trans . ToString (); } And the ConvertRowToJPropertyArray method looks like this: private static JProperty [] ConvertRowToJPropertyArray ( DataRow dr ) { JProperty [] o = new JProperty [ dr . Table . Columns . Count ]; for ( int i = 0 ; i & lt ; dr . Table . Columns . Count ; i ++) { o [ i ] = new JProperty ( dr . Table . Columns [ i ]. ColumnName , dr [ dr . Table . Columns [ i ]]); } return o ; In the next post I will consider a DataSet that contains multiple tables and relations. "},{"title":"Restart Ubuntu GUI without restarting Ubuntu","url":"/2009/09/restart-ubuntu-gui-without-restarting-ubuntu/","date":"2009-09-17 00:00:00 +0700","categories":["bash scriping","programming"],"body":"Here is a little trick I learned recently from Tropical Ice Cube . I am fairly new to Ubuntu and so all of what I heard was very new to me. So here is how you can “get out” of the graphical user interface, and restart it without having to reboot your machine. To start off with, press CTRL+ALT+F2 At this point your entire screen will become a terminal window, so to speak. Note: You can go back to your GUI by pressing: ALT+F7 Now, back in the terminal, you can find the Ubuntu GUI process by running the following command. Note: once you see the results, you might want to scroll up or down the screen, in which case you can hold SHIFT and page-up or page-down keys to do that. ps -A Look for the process Xorg and gdm in the list – these are running the GUI. Of couse, you can also pipe the output of ps -A to grep to filter out Xorg and gdm processes as follows: ps -A | grep Xorg ps -A | grep gdm You will probably notice that the output for Xorg is something like below. Note entry tty7 . This stands for teletype7 and harks back to the old unix days. For more information about tty, see Ubuntu Forums post on tty and pty. . You’ll see that It is no coincidence that CTLT+ALT+F7 brings you back to the GUI! 2899 tty7 00:59:39 Xorg Now run the killall command which terminates the specific processes as well as their children: killall Xorg gdm If you hit ALT+F7 now, there is no GUI! Where has it gone? Well you killed that process and in order to start it up again, run the following in the terminal: sudo gdm This will launch the GUI and take you back to the Ubuntu login screen. You will have to open all your GUI based applications again, of course. One thing also to note is that the GUI might start on a different tty. To check this type the following command in the terminal: who In my case, I see the following output. This tells me that there are two tty sessions running; one on tty2 (which is the terminal application I just used to restart gdm) and tty9 which is where the new gdm instance started. So now this basically means to get back to the GUI I need to hit CTRL+ALT+F9 . darren tty2 2009-09-17 10:29 darren tty9 2009-09-17 10:50 ( :0 ) "},{"title":"Transform a DataSet to JSON using Json.NET (Part 2)","url":"/2009/10/transform-a-dataset-to-json-using-json-net-part-2/","date":"2009-10-19 00:00:00 +0700","categories":["programming"],"body":"Here is how to transform a DataSet which contains multiple DataTable and DataRelations, well at least, one way you can do it! See Transform a DataSet to JSON using Json.NET (Part 1) for details on how to transform a single DataTable into JSON using Json.NET. **** Note that this is designed to work when loading one root row at a time. For example, if you have a database of people and there were related tables for work experience and work experience details then you would load one person and all their related work experience into the DataSet and then use this solution to convert it into JSON. The DataSet might be designed as follows: We start with the ConvertDataToJson() method which has the same principal purpose as the example in Part 1 except it calls another method GetRelatedData() to return the JSON. Therefore the ConvertDataToJson() is straightforward and looks as follows: private string ConvertDataToJson () { JObject data = new JObject ( GetRelatedData () ); return data . ToString (); } The GetRelatedData() method converts the related data into JSON. We first need to calculate the number of related DataTables in the DataSet. That gives us the size we need for our JProperty array. Then, as we loop through each DataTable, we set the name of each JProperty to the name of the DataTable. Note, however, that we check if ds.Tables[i].ParentRelations.Count is greater than 0 before creating a new JArray. This is because we only want to convert the related data into a JArray once and therefore have it be part of a related JSON element once. If the current table does not have any parent relations we create a JArray and using Linq we enumerate through each row in the table calling ConvertRowToJObject(r) and GetChildRows(r). Here is the complete method: private JProperty [] GetRelatedData () { //relatedDataArraySize can be passed into this class //it is calculated using the following formula //relatedDataArraySize = Total number of DataTable - Number of DataTable with DataRelations JProperty [] o = new JProperty [ relatedDataArraySize ]; int subtractor = 0 ; for ( int i = 0 ; i < ds . Tables . Count ; i ++) { if ( ds . Tables [ i ]. ParentRelations . Count > 0 ) { subtractor += 1 ; } if ( ds . Tables [ i ]. ParentRelations . Count == 0 ) { //Create a JPropery -> JArray o [ i - subtractor ] = new JProperty ( ds . Tables [ i ]. TableName , new JArray ( from r in ds . Tables [ i ]. AsEnumerable () select new JObject ( //See Part 1 for an explaination of the ConvertRowToJObject method ConvertRowToJObject ( r ), //See below for details of the GetChildRows method GetChildRows ( r ) ))); } } return o ; } In case your interested the code to calculate the relatedDataArraySize is: foreach ( DataTable t in ds . Tables ) { if ( t . ChildRelations . Count & gt ; 0 ) numberOfTablesWithRelations += 1 ; } The GetChildRows(DataRow) method is as follows. Notice that it is a recursive method so that it will keep calling itself until there are no more child rows for the current DataRow. private JProperty GetChildRows ( DataRow r ) { //Hack...need to create a JProperty even if there is no data //In your code you might want to remove this element either by a string relpace method JProperty o = new JProperty ( \"ignore\" , \"ignore\" ); //If there are child rows... if ( r . Table . ChildRelations . Count & gt ; 0 ) { if ( r . GetChildRows ( r . Table . ChildRelations [ 0 ]). Length & gt ; 0 ) { o = new JProperty ( r . Table . ChildRelations [ 0 ]. ChildTable . TableName , new JArray ( from rc in r . GetChildRows ( r . Table . ChildRelations [ 0 ]) select new JObject ( ConvertRowToJObject ( rc ), GetChildRows ( rc ) ))); } } return o ; } "},{"title":"Tagging in Mercurial","url":"/2009/11/tagging-in-mercurial/","date":"2009-11-22 00:00:00 +0700","categories":["bash scriping","programming"],"body":"Introduction If you are building software and want to release that software at regular intervals then why not tag each release in Mercurial? That way you can very easily rollback to that version of the software at any time. Whats more is that you can use your own tags rather than trying to remember the mercurial revision number that was current at the time of release. There are many approaches to software versioning which will not be covered here. Check out this wikipedia article on software versioning for more details on the different approaches. The purpose of this article is to apply the versioning as a tag in your mercurial repository. Apply a tag in mercurial When using mercurial, you must always tag after a commit. So if you know that you want to tag your current working directory, you must commit it first and then tag it. Given this it obviously makes it very easy to tag a commit that you might have made a long time ago in the past. Bear this in mind when using the tag command in mercurial and you will understand clearly how this feature can be used. This is how you would tag a revision in a mercurial repository, first decide what revision you want to tag and then run the following command. In this example it will tag revision number 45 with “2.0.5″ hg tag -r 45 2.0.5 This command automatically commits this tag, so if you run “hg tip” in your terminal, you will see something like the following: changeset: 51:209937087918 tag: tip user: Darren Jensen date: Sun Nov 22 13:10:47 2009 summary: Added tag 2.0.5 for changeset b50b86512aac Note that the changeset for “tip” is 51:209937087918 and the comment is “Added tag 2.0.5 for changeset b50b86512aac” (this comment is also added automatically by mercurial by the way). Given that I asked mercurial to apply the tag to revision 45, you can easily see that it is a commit made a while ago. The changeset indicated in the comment (“b50b86512aac”) does indeed refer to revision 45. Use a tag in mercurial So how to use this tag going forward? Here are a couple of ideas. Use it to see the commit info at that time using the log command as follows: hg log -r 2.0.5 This will return the log entry that is associated with that tag, so in our example it would return the entry for changeset 45:b50b86512aac, as follows: changeset: 45:b50b86512aac tag: 2.0.5 user: A Developer date: Tue Jul 21 14:27:50 2009 summary: Made some serious changes to the application and deployed it to production Use it to clone the repository to the tag point (note that the tag itself would not be included since that is added after this point). So if you do a “hg tip” on this cloned repository, then your would see the log entry for changeset 45 but the tag and the changeset that commit the tag will not be there (changeset 45 will have the tag “tip” in this case!). hg clone -r 2.0.5 somerepo Check out the Mercurial wiki for more details regarding tagging in mercurial , including how to deal with tagging conflicts, local tags (as opposed to “regular” tags which is covered in this article), removing tags and more. "},{"title":"Compare Amazon S3 to EBS data read performance","url":"/2009/12/compare-amazon-s3-to-ebs-data-read-performance/","date":"2009-12-30 00:00:00 +0700","categories":["amazon ec2 s3 cloud","bash scriping"],"body":"If you are going to store data in the cloud that requires frequent reading (such as a database file) then you should consider using EBS. Just to reinforce that statement, lets compare the read performance between S3 and EBS. I used the following straightforward Ruby program to perform the test. To try this yourself, just fill in the access keys and configure the local folder, bucket and files with your own setup: 1 require 'benchmark' 2 require 'rubygems' 3 require 'aws/s3' 4 5 AWS :: S3 :: Base . establish_connection! ( :access_key_id => YOUR_ACCESS_KEY_ID , :secret_access_key => YOUR_SECRET_ACCESS_KEY ) 6 7 Benchmark . bm do | x | 8 x . report ( \"disk (1 read): \" ) { 1 . times { File . read ( '/your-ebs-mount/1.xml' )}} 9 x . report ( \"s3 (1 read): \" ) { 1 . times { AWS :: S3 :: S3Object . value '1.xml' , \"your-s3-bucket\" } } 10 x . report ( \"disk (1000 read): \" ) { 1000 . times { File . read ( '/your-ebs-mount/1.xml' )}} 11 x . report ( \"s3 (1000 read): \" ) { 1000 . times { AWS :: S3 :: S3Object . value '1.xml' , \"your-s3-bucket\" } } 12 end This code was executed in a small EC2 instance on the cloud and the output is shown below. As you can see, disk read is not only faster (as expected) but very much faster at about 2000x!! Of course, S3 is best used for backing up files, packing up a EC2 image and as a source to Amazon’s CDN via CloudFront, which explains the massive speed difference here. The purpose of this test was to simply make a side-by-side comparison of reading the same file from S3 compared with EBS. user system total real disk ( 1 read ) : 0.000000 0.000000 0.000000 ( 0.000126 ) s3 ( 1 read ) : 0.010000 0.000000 0.010000 ( 0.308293 ) disk ( 1000 read ) : 0.030000 0.020000 0.050000 ( 0.116382 ) s3 ( 1000 read ) : 0.710000 0.100000 0.810000 ( 259.014794 ) "},{"title":"Remote desktop using a single tsclient terminal command","url":"/2009/12/remote-desktop-using-a-single-tsclient-terminal-command/","date":"2009-12-30 00:00:00 +0700","categories":["bash scriping"],"body":"If you use Ubuntu and have to work on remote computers that require using a GUI (like Windows machines for example), then you will find this following tip very useful! Firstly let me explain what you will have; after just a few minutes of minor configuration, you will be able to remote desktop into a Windows (or other) machine with just one swift command in the terminal! Say for example, you have a remote machine called “jungle” then wouldn’t it be nice if you could remote to it using the following single command? remotejungle I Thought so! OK, so here’s how! Firstly create a folder called .tsclient under your home directory (note the period before the name of the folder so that it is kept hidden by default). cd mkdir .tsclient Create a file (if not already) called .bash_aliases and save it under your home directory: cd touch .bash_aiases Now edit that file and add the alias for remoting into your remote machine, like so: alias remotejungle = 'tsclient -x ~/.tsclient/remotejungle.rdp' Note that this sets up the alias “remotejungle” so that it executes the command “tsclient -x ~/.tsclient/remotejungle.rdp” . As you can see it expects to find a file remotejungle.rdp in the .tsclients folder. In order to create this file you need to use the “Terminal Server Client” GUI application to setup your desired session settings for display screen size, color depth, remote sound, keyboard and so on and then click the “Save As” button and save the file as remotejungle.rdp to ~/.tsclient/. The final step is to modify the .bashrc file so that it picks up the alias list. Open up the .bashrc file in an editor and uncomment the following lines: if [ -f ~/.bash_aliases ] ; then . ~/.bash_aliases fi You’ll probably have to reload the .bashrc file again by opening a new terminal or executing the following command should also reload the .bashrc file: source .bashrc Now your good to go! Just try to remote into your computer using the alias that you setup during this tutorial and you will never be happier to remote into Windows from Ubuntu! "},{"title":"The easy way to create a new Ruby Gem!","url":"/2010/01/the-easy-way-to-create-a-new-ruby-gem/","date":"2010-01-11 00:00:00 +0700","categories":["ruby server programming"],"body":"If you want to make a Ruby Gem and want to do it in a very easy way then here’s is one very good option – use Jeweler ! Thanks to John Nunemaker at RailsTips for pointing me in the right direction with his post Building API Wrapping Gems Install git core apt-get install git-core Install jeweler gem gem install jeweler Now add the following line to your bashrc file to ensure that you always use spec (that if you do use spec). Otherwise the default is shoulda and if your happy using that then you can skip this part. export JEWLER_OPTS = \"--spec\" Setup your global username and email for git git config --global user.name \"Your Company Name\" git config --global user.email \"yourname@yourcompanyname.com\" Script your new gem using jeweler jeweler your shiny new_gem Set your init version . I do this so that I can easily generate the gemspec later. rake version:write MAJOR = 0 MINOR = 1 PATCH = 0 Modify the new gem rake file to add the meta info for your project [source lang=\"ruby\"] begin require ‘jeweler’ Jeweler::Tasks.new do |gem| gem.name = “your_shiny_new gem” gem.summary = %Q{A simple gem to access the Shiny API} gem.description = %Q{Use this gem to access our Shiny API !!} gem.email = “yourname@yourcompanyname.com” gem.homepage = “http://bitbucket.org/yourrepo/your\\ shiny_new gem” gem.authors = [\"Your Company Name\"] gem.add\\ development_dependency “rspec”, “>= 1.2.9″ # gem is a Gem::Specification… see http://www.rubygems.org/read/chapter/20 for additional settings end Jeweler::GemcutterTasks.new rescue LoadError puts “Jeweler (or a dependency) not available. Install it with: gem install jeweler” end Generate the gemspec rake gemspec Write the code that goes into your gem. This is where your own imagination comes in! See the Google Weather API Gem for an example. When your gem is finished (including all your tests passing!) move on to building and testing the installation of the Gem. ** ** code.write! Build the gem rake build Install the gem rake install You can deploy to github (the easiest solution when using Jeweler). However, you can still deploy to other repositories like Bitbucket for example. Then, as recommended by the author of Jeweler, Josh Nichols , have a delicious beverage! Happy Gem stone cutting! "},{"title":"Create a WCF Rest Application using .NET 3.5","url":"/2010/01/create-a-wcf-rest-application-using-net-3-5/","date":"2010-01-29 00:00:00 +0700","categories":["programming"],"body":"In this post, I will explain how to create a simple read only Rest service using WCF .NET 3.5 WebGet endpoints. I will also explain a little bit about the wonderful UriTemplate class. First create a new “WCF Service Application” Create a new “WCF Service Application” project. This will provide you with the standard example files for WCF that contain the string GetData(int value) and CompositeType GetDataUsingDataContract(CompositeType composite) signatures. Now this application is not yet Rest enabled. You need to do a few more things. No doubt, of course you can turn this into your own Visual Studio project template later on if you need to. Rest enable the WCF Application Go ahead and add a reference to System.ServiceModel.Web dll. Now add the WebGetAttribute on the method you want to expose. Let’s just use the supplied GetData method to keep things simple. However, notice that I have to change the data type of the input parameter to string since this is what the UriTemplate expects. This method signature in the IService1 Interface should look something like this: [OperationContract] [WebGet( UriTemplate = \"somepath/{value}\")] string GetData ( string value ); Then, in the web.config file that we create, we must also include the following entry (put this within the behaviors tag): <endpointBehaviors> <behavior name= \"WebBehavior\" > <webHttp /> </behavior> </endpointBehaviors> Now look within the services tag for the endpoint tag that describes the service you are building. You need to change the binding to “webHttpBinding” and add the attribute behaviorConfiguration=”WebBehavior” to this tag. The endpoint tag for your service should now look something like this: &lt; endpoint address=\"\" binding=\"webHttpBinding\" contract=\"WcfService1.IService1\" behaviorConfiguration=\"WebBehavior\" /> At this point you are ready to run the service, so hit F5. Your browser window should open to the service description page. Please then change the path following the “localhost” root to: ./Service1.svc/somepath/sqlserverdotnet and you should then see an XML response from the service as follows: &lt; string>You entered: never say never &lt; /string> Now you are free to modify this service method as you want. Perhaps to connect to a database and return a collection as serialized XML for example. Obviously you can remove the .svc extension from the url by using a HttpModule. There are many examples of url re-writing on the web which demonstrate exactly how to do this. "},{"title":"How to return content from an application running on a specific port in apache","url":"/2010/03/how-to-return-content-from-an-application-running-on-a-specific-port-in-apache/","date":"2010-03-14 00:00:00 +0700","categories":["programming"],"body":"Continuing on from the previous post where I explain how to show content from a different root folder depending on the sub domain e.g. http://foo.example.com will actually return content from http://example.com/foo (without changing the URL). Today I will extend this idea to show content from a separate service running on a different port within the same server. So in this case a call to http://foo.example.com returns http://example.com:port. As in the previous post, we assume that you already have your sub domain DNS settings correctly configured and the subdomain now points to the IP address of your server, you should do the following in Apache running on an Ubuntu server. Open the configuration file for your site. In this example, we will use the default site. nano /etc/apache2/sites-available/default Add the following additional VirtualHost configuration to the end of the file (change the port number on localhost if you need to, of course!) &lt; VirtualHost *:80> ServerName foo.example.com ProxyPass / http:// localhost:3000/ ProxyPassReverse / http:// localhost:3000/ ProxyPreserveHost on &lt; Proxy *> allow from all &lt; /Proxy> &lt; /VirtualHost> Now restart apache and your all done! /etc/init.d/apache2 restart "},{"title":"Step by step guide to setting up a client server workstation on Ubuntu and VirtualBox","url":"/2010/03/step-by-step-guide-to-setting-up-a-client-server-workstation-on-ubuntu-and-virtualbox/","date":"2010-03-19 00:00:00 +0700","categories":["programming"],"body":"I work on a number of different software projects. During the week I work on project called Yoolk and during the evenings and weekends I might work on AdFlickr or some other new project! Either of these main projects might also have sub-projects. The cleanest way to separate all of these projects is to develop the software using a guest OS running in VirtualBox. It makes sense, of course, that the guest OS matches your production environment as much as possible. Some of the benefits of doing this are: Complete separation of all your project dependencies. Change a setting on the guest development server for one project has absolutely no affect on your other projects. Allows much more freedom to experiment with different dependencies, try out new software and generally just screw around without worrying about having to clean up your mess after (just take snapshots or re-install your guest OS!). During development your are running your application in an environment which (at OS level at least) should be exactly the same as production. Therefore, this minimizes surprises during deployment. Allows you to keep your client OS the way you want it so that your development experience is just the way you like it. For example, installing different text editors, music libraries, video libraries, browsers etc etc without worrying that it will affect the application your building. While the benefits are great, there is a little work to be done to set this up initially (but it’s not too much). The longest step is installing the guest OS on VirtualBox. Obviously this depends on which guest OS you install. I use Ubuntu 9.04 Server Edition. By the way, my host OS is currently Ubuntu 9.10 but I know that this also works on Ubuntu 9.04 (since I used to run that before). Below is a step by step guide to setting this up. Notice that you will be alternating between the client (your desktop OS) and the server (the guest OS in VirtualBox): client : install server on virtualbox (make the default user the same name as client user name) client : configure virtualbox to use bridged networking and start server server : sudo apt-get update server : sudo apt-get install openssh-server server : ifconfig (and note the ip address) client : ssh into the server using the IP address you recorded in the previous step server : mkdir .ssh client : ssh-keygen (don’t enter any values, press return three times, yes passwords should be blank) client : cat ~/.ssh/id_rsa.pub – copy the output to the clipboard (very carefully, no pre/trailing white space) server : touch .ssh/authorized_keys server : sudo nano .ssh/authorized_keys – paste clipboard contents client : sudo nano /etc/hosts – add line: 192.168.???.??? myappname.dev www.myappname.dev (get ip from ifconfig) client : sudo /etc/init.d/networking restart (refresh new data in /etc/hosts) client : In nautilus go to ssh://myappname.dev then add the location to bookmarks client : In your web browser, go to http://myappname.dev and add the location to bookmarks (you will not get a response unless you have a web server running your application on your guest OS, of course!). After you have completed the above steps when logging in via ssh you shouldn’t be prompted for a password! Note, if IP address of the server changes, you should edit /etc/hosts to reflect that change. "},{"title":"How to subdomain redirect using Apache Virtual Host","url":"/2010/04/how-to-subdomain-redirect-using-apache-virtual-host/","date":"2010-04-14 00:00:00 +0700","categories":["programming"],"body":"Do you want to setup a website subdomain so that, for example, http://foo.example.com will actually, without the end user realizing, return content from http://example.com/foo ? In this short, but useful blog post, I will show you how you can redirect a subdomain to a server directory without anyone (except you) knowing! Assuming that you already have your subdomain DNS settings correctly configured and the subdomain now points to the IP address of your server, you should do the following in Apache running on an Ubuntu server. Open the configuration file for your site. In this example, we will use the default site. nano /etc/apache2/sites-available/default Add the following additional VirtualHost configuration to the end of the file. &lt; VirtualHost *:80> ServerName foo.example.com DocumentRoot /var/www/foo &lt; /VirtualHost> Now restart apache and your all done! /etc/init.d/apache2 restart If you point your browser to http://foo.example.com you will see the content from http://example.com/foo ! Note: You do, of course, have to create the folder /var/www/foo and put a index.html file there with some content like “Here be foo!” so you can test it actually works! Extra Bonus Script Time! As an added bonus, I have included a setup script that you can use to perform the above in one shot. Use it if you wish. Change foo and example.com to your domain and subdomain names. #!/bin/bash SITE_CONFIG_PATH = \"/etc/apache2/sites-available/default\" echo \"Configure default site with foo virtual host\" if ! grep -q \"ServerName foo.example.com\" $SITE_CONFIG_PATH then echo \"Configuring...\" echo \"\" >> $SITE_CONFIG_PATH echo \"# foo virtual host\" >> $SITE_CONFIG_PATH echo \"&lt;VirtualHost *:80>\" >> $SITE_CONFIG_PATH echo \" ServerName foo.example.com\" >> $SITE_CONFIG_PATH echo \" DocumentRoot /var/www/foo\" >> $SITE_CONFIG_PATH echo \"&lt;/VirtualHost>\" >> $SITE_CONFIG_PATH echo \"Restart apache\" /etc/init.d/apache2 restart else echo \"Already configured!\" fi "},{"title":"Hudson setup to execute scripts with root privileges (safely)","url":"/2010/05/hudson-setup-to-execute-scripts-with-root-privileges-safely/","date":"2010-05-14 00:00:00 +0700","categories":["programming"],"body":"There is a lot out there on the web about how to setup and install Hudson . In fact that process is very easy so I will just provide a script to explain that (see the end of this post). What I found to be an interesting challenge is getting Hudson to execute my scripts with root privileges… The first time I execute the script (without sudo), my job fails and I am informed that the user hudson does not have permission to execute apt-get. So naturally, I prefix the script with sudo and my job fails again, this time with an incorrect password. So how to get Hudson to execute my script with root privileges?. After a lot of searching I managed to find one blog post which mentions adding the user (in my case user hudson) to the sudoers file and giving the user full access without being promted for a password. Using visudo, I added this line to my sudoers file: hudson ALL =( ALL ) NOPASSWD:ALL However, I was most concerned about giving hudson the rights to do what it wanted without requiring a password as this leaves an obvious security hole. So I posted my concern on ServerFault.com and fortunately I was informed that it is possible to lock down the commands and the parameters that can be used by a user without requiring a password. So I changed my entry in sudoers file to this: hudson ALL =( ALL ) NOPASSWD:/var/scripts/the-script-I-want-to-run.bash Now all hudson can do (without being prompted for a password) is to execute sudo /var/scripts/the-script-I-want-to-run.bash. This makes be feel safe and my build works a treat each and every time too! I thought I would share this since I could not find a clear solution to this issue. By the way, if you want to install hudson on your linux server, here is the script I prepared earlier! #Install Hudson #NOTE: You MUST manually add hudson user to the sudoers account using visudo, #specifying all the scripts that you want hudson to be able to run without requiring a root password, for example: #hudson ALL=(ALL) NOPASSWD:/var/scripts/the-script-I-want-to-run.bash wget -O - http:// hudson-ci.org/debian/hudson-ci.org.key | sudo apt-key add - if ! grep -q \"http:// hudson-ci.org/debian binary/\" /etc/apt/sources.list ; then echo \"deb http:// hudson-ci.org/debian binary/\" >> /etc/apt/sources.list fi #NOTE: On a fresh install, you will have to run this script twice since the first time the script will not know about sources.list update apt-get install hudson #Install Hudson Plugins if test ! -f /var/lib/hudson/plugins/mercurial.hpi ; then wget -P /var/lib/hudson/plugins http:// hudson-ci.org/download/plugins/mercurial/1.28/mercurial.hpi chown hudson:nogroup /var/lib/hudson/plugins/mercurial.hpi fi Additionally, if you want Hudson to be accessible at http://hudson.yourserver.com while keeping Hudson running on port 8080 and you are running Apache then run the following script! SITE_CONFIG_PATH = \"/etc/apache2/sites-available/default\" if ! grep -q \"ServerName hudson.yourserver.com\" $SITE_CONFIG_PATH then echo \"Configuring...\" echo \"\" >> $SITE_CONFIG_PATH echo \"# Hudson virtual host\" >> $SITE_CONFIG_PATH echo \"&lt;VirtualHost *:80>\" >> $SITE_CONFIG_PATH echo \" ServerName hudson.yourserver.com\" >> $SITE_CONFIG_PATH echo \" ProxyPass / http:// localhost:8080/\" >> $SITE_CONFIG_PATH echo \" ProxyPassReverse / http:// localhost:8080/\" >> $SITE_CONFIG_PATH echo \" ProxyPreserveHost on\" >> $SITE_CONFIG_PATH echo \" &lt;Proxy *>\" >> $SITE_CONFIG_PATH echo \" allow from all\" >> $SITE_CONFIG_PATH echo \" &lt;/Proxy>\" >> $SITE_CONFIG_PATH echo \"&lt;/VirtualHost>\" >> $SITE_CONFIG_PATH #Create symlink to proxy module ln -s ../mods-available/proxy.load /etc/apache2/mods-enabled/proxy.load ln -s ../mods-available/proxy.conf /etc/apache2/mods-enabled/proxy.conf ln -s ../mods-available/proxy_http.load /etc/apache2/mods-enabled/proxy_http.load echo \"Restarting apache\" #Restart Apache /etc/init.d/apache2 restart else echo \"Already configured!\" fi "},{"title":"Using Google Analytics API in a Ruby on Rails project","url":"/2010/06/using-google-analytics-api-in-a-ruby-on-rails-project/","date":"2010-06-27 00:00:00 +0700","categories":["ruby server programming"],"body":"Ingredients A new Rails project The Garb gem A Google Analytics Account Preparation I assume you know about Rails already….Garb can be installed as follows: gem install garb Go to your Google Analytics account and fetch the **Account Id** and **Profile Id** that your interested in. You will find your Account Id on your account home page under the link &#8220;Edit account settings&#8221;, it is the number after the &#8220;UA-&#8221;. You will find your Profile Id, by going back to your account home page, finding the profile in the table, then click the &#8220;Edit&#8221; link in that row&#8230;.the Profile Id is at the top of this page&#8230;.phew! If you are not familiar with the Google Analytics API, then you will find the [Data Feed Query Explorer][2] tool useful. ### The Task: Record and display Unique Pageviews from a set list of pages Now for the fun part! Lets build something that records Unique Page Views from a list of pages in a local database. We need to create a model to store these pages and the unique pageviews. Change into your rails project directory and run the following scaffold script: script/generate scaffold page url:string unique_pageviews:integer Open up the pages controller that was just generated and add the following method, which sets the class instance variable @profile to an instance of your Google Analytics profile based on the Account Id and Profile Id. 1 def get_analytics_profile 2 Garb :: Session . login ( your_google_analytics_username , your_google_analytics_password ) 3 accounts = Garb :: Account . all 4 #Loop all Accounts for account_id 5 accounts . each do | account | 6 if account . id == your_google_analytics_account_id 7 #Loop all Profiles for profile_id 8 account . profiles . each do | profile | 9 if profile . id == your_google_analytics_profile_id 10 @profile = profile 11 break 12 end 13 end 14 end 15 end 16 end Basically, Garb::Session.login is called passing your username and password to authenticate the session. Next we fetch all accounts and iterate the array until we find the account that matches your Account Id. Then we need to iterate all the profiles within that account until we find the profile that matches your Profile Id. Now add this method which will update all the unique pageviews in your list of pages: 1 def update_all 2 @pages = Page . all 3 @pages . each do | page | 4 get_analytics_profile if ! @profile 5 report = Garb :: Report . new ( @profile ) 6 report . metrics :unique_pageviews 7 report . dimensions :page_path 8 report . filters :page_path . contains => page . url 9 page . unique_pageviews = report . results . first . unique_pageviews 10 page . save 11 end 12 end 13 end Since you have used a scaffold to create this controller, there should already be an index method present. Add a call to the *update_all* method at the top of the index method so that everytime the index method is called the *update_all* method is called. Here is how your index method should look: 1 def index 2 update_all 3 @pages = Page . all 4 5 respond_to do | format | 6 format . html # index.html.erb 7 format . xml { render : xml => @pages } 8 end 9 end ### Final Touches Now all that remains is a little configuration, add some fixtures and we&#8217;re done! You should already have fixtures for your page model. Note that the url does not need to include the domain name, so **/pricing/** or **/sign-up/** are the sorts of values you should put here (obviously ones that exist in your Analytics profile!). The configuration is simply to add: 1 config . gem \"garb\" in the appropriate place of your environment.rb file. ### Get those metrics! Now fire up your rails app and navigate to the index for your pages http://localhost:3000/pages/. The call to **update_all** will be made which will update your local database with the metrics and then render the data in the browser in the usual Rails table format. ### Conclusion Using the [Garb gem][1] makes it extremely easy to work with the Google Analytics API in a Rails project. Obviously this example application can be improved to include caching, error handling (it will bomb out if the Id&#8217;s or urls are not found in your Analytics account) as well as other improvements that you might want to make. "},{"title":"What should the first blog post be about?","url":"/2010/09/what-should-the-first-blog-post-be-about/","date":"2010-09-08 00:00:00 +0700","categories":["ramblings"],"body":"It’s always exciting when something new arrives and the same goes for this shiny new blog of mine! I am so excited I just need to write something down now! I think the most appropriate thing would be to introduce myself. I have been working professionally in the wonderful wide world of Information Technology since graduation in late 2000. The first part of my professional life was spent in London working for exciting (but short lived) Web Design startups like Oven Digital. I lasted only 1 year in London before leaving to find work in Asia which I had auspicious feelings about ever since my gap year experience in India in 1993. This time, however, I went to South East Asia! When I arrived in Thailand, I was pleasantly surprised by how modern it was (especially since I spent the previous two months in Western African countries like Ivory Coast, Burkina Faso and Mali – which are all beautiful, but under developed places). However, I did not see myself settling in Bangkok. I wanted somewhere a little quieter and a place where I could be free to explore without needing too much money. So I hopped over the border to Cambodia. This was in January 2002. I met some very interesting people in Cambodia from many backgrounds and I ended up teaming up with someone for my first business venture (more on that later!). I stayed in Cambodia until 2007 and returned to London (with my Cambodian wife) to try to understand where I wanted my life to go. I decided that I wanted to focus on starting my own businesses and see them grow and succeed as well as helping others to grow and succeed within them! Since I was married to a Cambodian and had many contacts in Cambodia I decided to return in 2009. About the same time as my return to Cambodia in 2009, I started a blog called Never Say Never. This blog is a continuation of the Never Say Never blog and thus will be ramblings of what I discover in Technology, Business and Marketing while working for myself here in the Kingdom of Cambodia! Enjoy! "},{"title":"Let the social networking diaspora begin!","url":"/2010/09/let-the-social-networking-diaspora-begin/","date":"2010-09-15 00:00:00 +0700","categories":["programming"],"body":"So today is the day that Diaspora begins! I am looking forward to pulling it apart and seeing what these guys have done! I am sure that they are pretty nervous today about this release since all the hype and money surrounding this project means they must be feeling just a little weight on their shoulders right now! Regardless of how it is written, if it works well, I plan to use this in Cambodia to develop a free social networking site. I probably will seek donor funding to do this as it is more of a community and research project than a business project. The great thing about using Diaspora is being able to host your own node, which means bringing huge benefits to countries like Cambodia where Internet speeds are not always so reliable. Instead of building your profile data silo on servers based in USA we can provide the servers locally in Cambodia. It's amazing how people will migrate from one site to another when the speed is just that little bit faster and the responsiveness is that little bit snappier. I wonder if there will be support for multiple language interfaces in Diaspora too? I'll just have to wait a few more hours to find out! "},{"title":"Time to please Facebook not Google","url":"/2010/09/time-to-please-facebook-not-google/","date":"2010-09-15 00:00:00 +0700","categories":["marketing","programming"],"body":"I recently read a Wired article about how the Web is Dead and it got me thinking more about the role businesses play when marketing on the web. For a very long time now the Web has been the place to be if you want your business reach out to more customers. Search Engine Marketing has played a very important role in this as well as advertising via Google AdWords. But times, are changing fast! Over the past year or so there have been rumors spreading that the days of pleasing Google are over. By this I mean traditional SEO and SEM (AdWords). What businesses now need to do is please the likes of Facebook but they need to do more than just create a profile page for their business. Since Facebook is a walled garden, safely protecting its users from the harsh world of the rest of the web, businesses who do not join the Facebook party will be left cold on the other side of this fence. The answer lies in making applications for Facebook. If you can make something yourself or outsource the development of an app, then you stand a better chance of being found in Facebook. The app needs to be engaging, easy to use,  provide some value to the user and, of course, be free! What you need to do with your app is to generate a ROI you need to convert the users of the app into customers! Here is an example of what I mean. The Canada Yellow Pages App is great example of engaging the user in the app and bringing value to their customers and thereby, no doubt, increasing their bottom line. I plan to make some business apps that will distribute content on various social networks and will keep the progress of this project updated here on Tweetegy. "},{"title":"command to ftp in linux bash script","url":"/2010/11/command-to-ftp-in-linux-bash-script/","date":"2010-11-27 00:00:00 +0700","categories":["bash scriping"],"body":"Command line ftp scripts are often unappreciated. Often, when a requirement to ftp files arises it makes sense to write a ftp command file which automates this. The question is what is the best approach? Out of the box, most Linux and Unix ftp commands are limited. In particular, recursive folder creation on the ftp server is not possible with the basic linux / unix ftp tool. It lacks a command in ftp that offers this recursive functionality. So what is the solution if you require a bash script ftp for upload of files and folders? I recommend you download ,install and use ncftp . This is a far superior option compared with the basic set of ftp commands that comes with the installed ftp program with linux. It would not be fair to leave without providing an example so here is one! Imagine that you have a set of nested folders, some which are empty and some which contain files. You want to ftp the entire folder structure (including the files, of course!) to a remote ftp server. Here is a ftp bash script to do just that (replace with your settings where appropriate). FTP_HOST = ftp.yoursite.com FTP_LOGIN = yourusername FTP_PASSWORD = yourpassword ncftp & lt ;& lt ; EOF open -u $FTP_LOGIN -p $FTP_PASSWORD $FTP_HOST cd somefolder/onyourserver lcd \"somefolder/onyourclient\" put -R * bye EOF Save the above script in a file named ftp-example.sh and execute it in your terminal as you would any shell script. The key command in this example which creates all the folders recursively is * put -R * *. The rest of the example is pretty straight forward. "},{"title":"Install WordPress on Ubuntu 10.04 Slicehost","url":"/2010/12/install-wordpress-on-ubuntu-10-04-slicehost/","date":"2010-12-14 00:00:00 +0700","categories":["programming"],"body":"Here is a quick step by step for setting up a new Slicehost slice and installing WordPress on that slice. In this example we will use a Ubuntu 10.04 (Lucid Lynx) 32-bit 256 MB slice. The first part is a condensed version of these instructions. The second part introduces a bash script that will install and configure a LAMP stack in order to run WordPress. Part 1: Fire up and configure your new Slice! Goto slicehost , sign up, and pick your slice by selecting “Ubuntu 10.04 LTS (lucid) 32-bit” from the Linux Distribution drop down menu. Wait for the slice to fire up and then when it is marked active in your control panel you can ssh into the slice using the root username and password. ssh root@184.106.xxx.xxx First change the password of root passwd Add a new user and group. Follow the instructions on the screen. adduser demouser Open visudo visudo Add this line to the bottom of the file: demouser ALL =( ALL ) ALL On your LOCAL machine generate an ssh key mkdir ~/.ssh ssh-keygen -t rsa Copy this key to your new slicehost slice using scp on your LOCAL machine scp ~/.ssh/id_rsa.pub root@184.106.xxx.xxx: Back on the SERVER – your new Slicehost slice, run the following commands. This simply moves your local machine public key to the authorized_keys file for demouser. mkdir /home/demouser/.ssh mv id_rsa.pub /home/demouser/.ssh/authorized_keys chown -R demouser:demouser /home/demouser/.ssh chmod 700 /home/demouser/.ssh chmod 600 /home/demouser/.ssh/authorized_keys Make ssh more secure by editing /etc/ssh/sshd_config and changing the following three properties: Port 30000 <--- change to a port of your choosing PermitRootLogin no PasswordAuthentication no nano /etc/ssh/sshd_config Load custom rules into your server iptables. The easiest thing to do is copy paste the Slicehost iptables rules to a new file /etc/iptables.up.rules and then load these rules. Remember to open the port that you selected in the previous step for ssh. nano /etc/iptables.up.rules Now run the following to update rules from the new file /sbin/iptables-restore & lt ; /etc/iptables.up.rules You want to ensure that these rules are loaded on every instance boot. So create a new file /etc/network/if-pre-up.d/iptables and add the following script into this file: #!/bin/sh /sbin/iptables-restore & lt ; /etc/iptables.up.rules Run this to make the file executable chmod +x /etc/network/if-pre-up.d/iptables Reload SSHD /etc/init.d/ssh reload Now you can try logging in as the new user (demouser in this exampe) from your LOCAL machine ssh -p 30000 demouser@184.106.xxx.xxx If all goes well, you can log out of root at this point and continue to use demouser going forward. If you do get locked out, Slicehost have a browser based console, which you can use to attempt to fix any issues with SSH. In Part 2 we will prepare a bash script for installing WordPress. "},{"title":"A new JavaScript UI Framework is born: YoolkUI","url":"/2010/12/a-new-javascript-ui-framework-is-born-yoolkui/","date":"2010-12-15 00:00:00 +0700","categories":["programming"],"body":"Quickly! Name 5 JavaScript UI Frameworks! OK Good! Now add YoolkUI to that list! Notice, I am not suggesting to put it to the top of the list…..yet. YoolkUI is a brand new (well, 18 months in the making thus far), JavaScript UI Framework which has been hand crafted from the ground up right here in Cambodia! The original idea was to base the Framework on the popular GTK API so that it would be possible to build RI Applications using Glade . However, like most things, the project went on its own separate way and has some of its own specific components that you will not find in GTK. The ultimate dream, however, would be to create an application like Glade that could be used to quickly build layouts using YoolkUI. By the way, YoolkUI is 100% Open Source and completely free to use under MIT or GPL license . Here are a couple of screen captures of an application built using YoolkUI. If you want to see more, check out the YoolkUI website . "},{"title":"Clone a VirtualBox VDI using VBoxManage on the command line","url":"/2010/12/clone-a-virtualbox-vdi-using-vboxmanage-on-the-command-line/","date":"2010-12-20 00:00:00 +0700","categories":["bash scriping"],"body":"I run into this situation all the time. I want to try out a new software (usually it’s a database of somesort) and I want to try it on a clean server installation. I don’t want to mess up my client or an existing server or have to install the OS fresh each time, so I turn to VirtualBox cloning via the command line. Let’s spec out what we want the script to do: Clone a clean install VDI that exists in a known location (typically ~/.VirtualBox/HardDisks/) Create a new virtual machine Attach the new VDI to that machine Setup a standard configuration for boot order, bridged networking etc Fire up the instance Here is the final script. Note it includes a param to set the name of the new machine (not tested with spaces in the name yet). Note also, I have set my network to use wlan0. You might want to use a different network adapter. #!/bin/bash declare EXISTING_VDI = \"Ubuntu_10.04_Clean_Install.vdi\" declare CLONE_NAME = \"Ubuntu_10.04_Clean_Install_Clone\" if [ \"$#\" -eq 1 ] ; then CLONE_NAME = \"Ubuntu_10.04_\" $@ fi echo \"Cloning from: \" $EXISTING_VDI echo \"The new VDI name: \" $CLONE_NAME VBoxManage clonevdi ~/.VirtualBox/HardDisks/ $EXISTING_VDI ~/.VirtualBox/HardDisks/ $CLONE_NAME .vdi VBoxManage createvm -name $CLONE_NAME -register VBoxManage modifyvm $CLONE_NAME --hda ~/.VirtualBox/HardDisks/ $CLONE_NAME .vdi VBoxManage modifyvm $CLONE_NAME --nic1 bridged VBoxManage modifyvm $CLONE_NAME --bridgeadapter1 wlan0 VBoxManage modifyvm $CLONE_NAME --boot1 disk VBoxManage startvm $CLONE_NAME "},{"title":"Install WordPress on Ubuntu 10.04 Slicehost (Part 2)","url":"/2010/12/install-wordpress-on-ubuntu-10-04-slicehost-part-2/","date":"2010-12-20 00:00:00 +0700","categories":["bash scriping","mysql"],"body":"In Part 1 we setup a new virtual host instance using Slicehost. The steps can be pretty much the same for any new Ubuntu server. It’s just that in this case we are using 10.04 on Slicehost. Once you have the slice up and running then you probably want to do an update and then run a simple script to install and configure WordPress. Here is a nice script that will install WordPress and add a couple of plugins as a bonus! Note there is a second SQL script to setup the db that is required. It is shown after the first script below. You’ll probably want to replace YOUR-USERNAME and YOUR-PASSWORD with your own values! Note also that the name of the MySQL database is wordpress (which you can also change if you like) #!/bin/bash ############################################################# ## INSTALLS AND CONFIGURES WORDPRESS ON SERVER ## ## ## ############################################################# #Packages apt-get -y install mysql-server php5 libapache2-mod-php5 php5-xsl php5-gd php-pear libapache2-mod-auth-mysql php5-mysql #Install WordPress if [ ! -d /var/www/wp-content ] ; then echo \"#############################\" echo \"## START INSTALL WORDPRESS ##\" echo \"#############################\" SCRIPT_DIR = ` dirname \"$0\" ` WORDPRESS_PATH = \"/var/www\" mysql -u root --password = \"YOUR-PASSWORD\" & lt ; $SCRIPT_DIR /setupdb.sql #Set up wordpress directories [ ! -d $WORDPRESS_PATH ] & #038;&#038; mkdir $WORDPRESS_PATH #Download and install wordpress. Here is an alternative: https://help.ubuntu.com/community/WordPress mkdir $SCRIPT_DIR /downloads wget http://wordpress.org/latest.tar.gz -O $SCRIPT_DIR /downloads/latest.tar.gz tar -xzvf $SCRIPT_DIR /downloads/latest.tar.gz --directory = $SCRIPT_DIR /downloads/ mv $SCRIPT_DIR /downloads/wordpress/wp-config-sample.php $SCRIPT_DIR /downloads/wordpress/wp-config.php mv $SCRIPT_DIR /downloads/wordpress/* $WORDPRESS_PATH #Remove the default index.html file rm /var/www/index.html #Add our database settings to wp-config.php sed -i 's/database_name_here/wordpress/g' \"$WORDPRESS_PATH/wp-config.php\" sed -i 's/username_here/YOUR-USERNAME/g' \"$WORDPRESS_PATH/wp-config.php\" sed -i 's/password_here/YOUR-PASSWORD/g' \"$WORDPRESS_PATH/wp-config.php\" #Install our theme unzip $SCRIPT_DIR /theme.zip -d $WORDPRESS_PATH /wp-content/themes/ #Install some plugins (G-Lock Opt-in and Google Sitemaps) wget http://downloads.wordpress.org/plugin/google-sitemap-generator.3.2.3.zip wget http://downloads.wordpress.org/plugin/g-lock-double-opt-in-manager.zip unzip google-sitemap-generator.3.2.3.zip -d /var/www/wp-content/plugins/ unzip g-lock-double-opt-in-manager.zip -d /var/www/wp-content/plugins/ #Enable mod_rewrite cd /etc/apache2/mods-enabled ln -s ../mods-available/rewrite.load rewrite.load #Clean up rm -rf $SCRIPT_DIR /downloads echo \"###########################\" echo \"## END INSTALL WORDPRESS ##\" echo \"###########################\" fi Here is the setupdb.sql script that is used in the above installation script. CREATE DATABASE wordpress ; GRANT ALL PRIVILEGES ON wordpress . * TO \"YOUR-USERNAME\" @ \"localhost\" IDENTIFIED BY \"YOUR-PASSWORD\" ; FLUSH PRIVILEGES ; What about Apache configuration I hear you cry! Let's put that in Part 3 shall we?! "},{"title":"So this is how Chrome will take over our desktops!","url":"/2010/12/so-this-is-how-chrome-will-take-over-our-desktops/","date":"2010-12-24 00:00:00 +0700","categories":["ramblings"],"body":"Have you ever used a themed version of Chrome in full screen mode? If not you should give it a try! It almost looks and feels like a traditional desktop from say Windows Vista or XP (I thought long and hard about which way to include these in the text! #seo). If Chrome engineers turn the tabs into windows that can be moved around within the parent Chrome window then they will have hijacked our desktop (or at least, the user has willingly let them hijack the desktop by pressing F11). Imagine using a whole host of Rich Internet Applications via Chrome this way! Here is a screen capture of my Chrome browser as it might appear as a full desktop. You can see my Focus Booster App is open and always above my desktop. It looks great with the downloaded chrome distance theme too! "},{"title":"Install MongoDB and load dummy logging data using MongoMapper","url":"/2011/01/install-mongodb-and-load-dummy-logging-data-using-mongomapper/","date":"2011-01-03 00:00:00 +0700","categories":["databases","nosql","ruby server programming"],"body":"It turns out to be very easy to install Mongo DB on Ubuntu , so I will not go into detail too much here. If you follow these instructions then MongoDB will start after the installation completes and you can try it out at the console as follows (Note: The first command will fire up the Mongo DB shell and the second will list all the databases): mongo show dbs The shell is probably the best way to get started with MongoDB. Here are some more details on using the MongoDB shell . Or just type help in the console! Another useful trick to note is that it’s possible to get some administrative details about the server by pointing your browser to the following location: http://localhost:28017/ . This displays details like version number, database stats, namespace information etc. Loading MongoDB with test data. There are three ways to insert data directly into MongoDB. There is the standard insert command which inserts one document at a time, there is the batch insert which can be used to insert multiple documents at a time (this is more efficient than inserting one at a time), or there is the mongoimport command line utility for running a raw data import from a relational database like MySQL. Many of these I will cover in future posts. There also are indirect ways to load data namely via database language adapters like MongoMapper for Ruby. Here is an example of generating random data and loading it into MongoDB: 1 require 'rubygems' 2 require 'mongo' 3 require 'mongo_mapper' 4 require 'faker' 5 require 'WebAppVisitorLog' 6 7 #Create a couple of types of documents (see the classes) 8 MongoMapper . database = \"mongodb-logging\" 9 10 WebAppVisitorLog . destroy_all 11 12 10000 . times { | n | 13 user_id = ( 1 . . 1000 ) . to_a . rand 14 session_id = ( 1 . . 10000 ) . to_a . rand 15 updated_at = ( 1 . . 35 ) . to_a . rand . days . ago 16 17 @log = WebAppVisitorLog . create ( 18 { 19 :session_id => session_id , 20 :user_id => user_id , 21 :updated_at => updated_at 22 }) 23 @log . save () 24 } Here is the code for the WebAppVisitorLog class: 1 class WebAppVisitorLog 2 include MongoMapper :: Document 3 4 key :session_id , Integer 5 key :user_id , Integer 6 end "},{"title":"Using MongoDB as a logging database with a Rails 3 application","url":"/2011/01/using-mongodb-as-a-logging-database-with-a-rails-3-application/","date":"2011-01-05 00:00:00 +0700","categories":["databases","nosql"],"body":"Here is a great article on the MongoDB site that explains how to setup Rails 3 to work with MongoDB with very little effort! Essentially this means doing the following things. Create a new Rails 3 application issuing the –skip-active-record switch rails new my_app --skip-active-record Add the following line to include mongo_mapper to your gemfile gem \"mongo_mapper\" Now run the bundle installer bundle install Finally, add this initializer to your Rails 3 project to connect to your MongoDB app: 1 MongoMapper . connection = Mongo :: Connection . new ( 'localhost' , 27017 ) 2 MongoMapper . database = \"#myapp- #{ Rails . env } \" 3 4 if defined? ( PhusionPassenger ) 5 PhusionPassenger . on_event ( :starting_worker_process ) do | forked | 6 MongoMapper . connection . connect_to_master if forked 7 end 8 end Now it’s time to create our dummy action: rails g controller DummyAction simulate Creating models that use MongoMapper is very easy. This is a good thing because, at the time of wrting, Rails 3 generators for MongoMapper are not included, so you might end up trying what I did and getting “No value provided for required options ‘–orm’” error and then trying all combinations under the sun for the –orm switch to get the generator to work! If you want to use generators then try Rails 3 generators , which is a project that simply has: “Rails 3 compatible generators for gems that don’t have them yet” So I created my own models by hand because it is so easy to do. A typical MongoMapper model might look like this WebAppVisitorLog class of mine: 1 class WebAppVisitorLog 2 include MongoMapper :: Document 3 4 key :session_id , Integer 5 key :user_id , Integer 6 end Finally, what to do with the action? Well push a log entry to MongoDB like so: 1 class DummyActionController & lt ; ApplicationController 2 def simulate 3 #log some fake data in Mongo DB 4 @log = WebAppVisitorLog . create ( 5 { 6 :session_id => 1 , 7 :user_id => 1 , 8 :updated_at => Time . now 9 }) 10 @log . save () 11 end 12 end Be sure to add this to your config.rb file also: 1 get \"dummy_action/simulate\" "},{"title":"(Almost) Everthing an experienced programmer needs to know to get started with PHP","url":"/2011/01/almost-everthing-an-experienced-programmer-needs-to-know-to-get-started-with-php/","date":"2011-01-24 00:00:00 +0700","categories":["php","programming"],"body":"I consider myself to be an experienced programmer. Having spent the last 10 years working on a variety of platforms like Windows and Linux, using different programming languages like Java, C#, VB, Ruby, JavaScript etc, I feel that I have a broad experience and understanding of programming. However, there is always that moment when one must use a different language, for whatever reason. In my case I had to start tinkering with a site developed by someone else in PHP. Now I have always tried to avoid PHP since I was very much into compiled languages like Java and C#. Since breaking away from the corporate world, I have been able to move to scripting languages like Ruby, JavaScript and now PHP. What I want to do is briefly summarize what an experienced programmer needs to know to get started with PHP. I managed to build a simple site in PHP in less than one week using PHP / MySQL / Apache on Linux (commonly known as LAMP stack). I will just list some of the things that I found useful during the journey. The site I built by the way is for my very own Computer School in Cambodia (shameless personal project plug, I suppose, but only if your in Cambodia . Note: To place PHP script inside a page you need to open with <? and close with ?> Setting variables: $somevar = \"hello world\"; String concatenation is done with dots! $url=\"/somthing/\".$id.\"/\".$name.\".html\"; Connecting to a database mysql_connect(localhost,$username,$password); mysql_select_db($database) or die( \"Unable to select database\"); Execute a query and count results $results=mysql_query($query_sql_string); $num_results=mysql_numrows($results); Get a value from the resultset //Note this can be in a while loop. //$i is a counter. $id=mysql_result($results,$i,\"id\"); IF statements if (!is_null($id)) { // do something } Output values in HTML rendering <h1 align=\"center\"> <? = $some_value ?> </h1> Date formatting More details of date formatting in PHP $date_obj->format(\"D j M Y\") Server Side Includes include('db.php'); "},{"title":"Setting join table attribute :has_many :through association in Rails ActiveRecord","url":"/2011/02/setting-join-table-attribute-has_many-through-association-in-rails-activerecord/","date":"2011-02-10 00:00:00 +0700","categories":["rails appplication programming"],"body":"I needed to set an attribute on a join table or in ActiveRecord, the* :through* model as an atomic save action. Let’s set the scene: I have a Product *and a *User *which has a many-to-many relationship since a *Product *can have many *Users *and a User can have many *Products . Typically, in a database we might have a table product , user and product_user . In Rails I created an association class called Collaborators *since it fit my domain better. Additionally the *Collaborators *class has one attribute *is_admin which indicates the User *(or *Collaborator *actually) is an administrator for the particular *Product . One of the business rules in the application was that if a User *creates a new *Product *then they are automatically an administrator for that *Product . My dilemma was how to create the new Product , associate it correctly with the User *\\ and* set* is_admin* to true all in one nice and neat command? The answer, it turns out, is straightforward and elegant. I am fairly new to Rails myself, so I understand if this might appear a little overwhelming if you are also just starting out. I have included a set of useful references at the end of this post to help you on your way. 1 class Product & lt ; ActiveRecord :: Base 2 has_many :collaborators 3 has_many :users , :through => :collaborators 4 end 1 class User & lt ; ActiveRecord :: Base 2 has_many :collaborators 3 has_many :products , :through => :collaborators 4 end 1 class Collaborator & lt ; ActiveRecord :: Base 2 belongs_to :product 3 belongs_to :user 4 end Once you have the above ActiveRecord models setup all you need to do is execute the following code to save a new Product , associate a User *as a *Collaborator *and set them as an administrator for the *Product . 1 #assume we have the product and current_user instance already set 2 product . save & #038;&#038; product.collaborators.create(:user => current_user, :is_admin => true) Useful Resources and Docs for ActiveRecord Associations Has Many Through Association ActiveRecord::Base Documentation "},{"title":"How to get a semantic list of all countries and locations in the world","url":"/2011/02/how-to-get-a-semantic-list-of-all-countries-and-locations-in-the-world/","date":"2011-02-18 00:00:00 +0700","categories":["semantic application programming"],"body":"I am in the process of building a semantic web application. Since the application I am building will be used globally, I wanted to start with including a semantic collection of all countries in the world. It turns out to be rather simple thanks to the GeoNames API. I am a big fan of JSON so all the examples I give here will return the results as JSON. Complete list of all Countries Get a complete list of all countries is as simple as making the following request (note in all the examples the username is demo . In a real application you would need to create your own account with GeoNames): http://api.geonames.org/countryInfoJSON?username = demo Get data for a single Country If you want data for a single country then you must include the 2 letter ISO_639 code in your request. In the following example, the request returns data for Cambodia (iso_639 code KH): http://api.geonames.org/countryInfoJSON?country = KH & #038;username=demo By the way, you will notice that the GeoNames API contains a rich set of data . Locations within a Country GeoNames API has the ability to drill down into various administrative levels (aka populated places ) within the country via the GeoNames Children API . To get all the children of a particular country you need to first get the geonameId from GeoNames Country Info API (discussed above already), and pass that as a parameter to the Children API. The following example gets the first level of children for Cambodia (which happens to have a geonameId of 1831722) http://api.geonames.org/childrenJSON?geonameId = 1831722 & #038;username=demo To drill down into the next level of children you recursively pass in the geonameId of each child until the API returns an empty result. If you try the GeoNames API with UK data then the first level of children are England, Scotland, Wales etc then if you pass the geonameId of England you will get all the counties of England like Sussex, Kent, Essex, Nottinghamshire etc. Continue to pass in the geonameId until there are no more children and you will have all the locations in the UK (at least according to GeoNames). I hope you find this useful. In my next post I will discuss how to get language data from dbpedia using their RESTful API or a custom SPARQL query. "},{"title":"How to get a semantic list of all spoken languages in the world","url":"/2011/02/how-to-get-a-semantic-list-of-all-spoken-languages-in-the-world/","date":"2011-02-22 00:00:00 +0700","categories":["semantic application programming"],"body":"First stop, grab and parse the list from here: http://en.wikipedia.org/wiki/List_of_ISO_639-1_codes . Getting language meta data by url dereferencing This is the easiest method but has less control over the data that is returned (by default all data about the language is returned). Simply replace the [language_name] part of the following url with the value in “Language Name” (NOTE: it is case sensitive so the Language Name must be in capital case): http://dbpedia.org/resource/ [ Language_Name ] _language.json So for Aramaic the url would be: http://dbpedia.org/data/Aramaic_language.json For Khmer the url would be: http://dbpedia.org/data/Khmer_language.json Getting a subset of data Let’s say we want a little more control and therefore need only a subset of the data available. In this case we can turn to the SPARQL endpoint available at http://dbpedia.org/sparql . Copy and paste the following example to get name, abstract, depiction, comment and label data for English Language: PREFIX dbprop : & lt ; http : // dbpedia . org / property /& gt ; PREFIX dbowl : & lt ; http : // dbpedia . org / ontology /& gt ; PREFIX dbpedia : & lt ; http : // dbpedia . org / ontology /& gt ; PREFIX rdfs : & lt ; http : // www . w3 . org / 2000 / 01 / rdf - schema #& gt ; PREFIX foaf : & lt ; http : // xmlns . com / foaf / 0 . 1 /& gt ; SELECT ? uri ? name ? label ? thumbnail ? depiction ? abstract ? comment WHERE { ? uri dbprop : name \"English\" @ en ; rdf : type dbpedia - owl : Language ; dbprop : name ? name ; foaf : depiction ? depiction ; dbowl : thumbnail ? thumbnail ; dbowl : abstract ? abstract ; rdfs : comment ? comment ; rdfs : label ? label . filter ( lang ( ? label ) = \"en\" ) filter ( lang ( ? abstract ) = \"en\" ) filter ( lang ( ? comment ) = \"en\" ) } Simply change ?uri dbprop:name \"English\"@en; to the (English) name of the language that you want to search for e.g. ?uri dbprop:name \"French\"@en; will return the dbpedia data fields related to the French Language. Resources SPARQL: http://sparql.org/ Linked Data: http://linkeddata.org/ "},{"title":"Moving from Apache to Cherokee Web Server","url":"/2011/04/moving-from-apache-to-cherokee-web-server/","date":"2011-04-05 00:00:00 +0700","categories":["bash scriping"],"body":"I recently decided to move a website of mine from Apache to Cherokee. Why? Because Cherokee is fast and Google likes fast sites. Apparently, Cherokee is the fastest web server in town . I first read about Cherokee in Linux Format magazine and decided to give it a shot with my AdFlickr site which I have running on a Linode instance (Ubuntu 10.04 by the way). Installing Cherokee on Ubuntu 10.04 Fortunately, it is really easy to install Cherokee on Ubuntu 10.04 since version 0.99.39 is available in the software repositories. Here is basically what I run in my server.bash file to install Cherokee, PHP and MySql: apt-get install cherokee cherokee-doc libcherokee-mod-libssl libcherokee-mod-streaming libcherokee-mod-rrd php5-cgi php5 php5-mysql php5-xsl php5-gd php-pear curl libcurl3 libcurl3-dev php5-curl mysql-server Tunnelling into the Cherokee Admin Cherokee includes a web admin interface for configuring the server. To get that running you need to start it up on your server and then use SSH tunnelling to view in a local browser. So on your server run the following to start the Cherokee Admin deamon: cherokee-admin -b & Then on your local machine, setup the SSH tunnel as follows: ssh -L 9090:localhost:9090 mysite.com -N Now, navigate to http://localhost:9090 and log in using the username and password shown in the output on your server console. You should be greeted with the Cherokee Admin home screen! Configuration of Cherokee is a breeze with the use of Wizards. Just navigate to your Virtual Server Behaviour tab, click on wizards button and select the appropriate category. For PHP select Languages then Run Wizard next to PHP. Specific issues when moving from Apache I had two issues, one was not having installed php5-cgi which I simply added to my apt-get install list. The second issue was related to url rewriting (e.g. all the rewrite rules I had in my .htaccess file). I found that the syntax did not exactly copy accross to Cherokee. I also found that in Web Admin interface did not preserve the ordering of the rules, so I decided to edit the cherokee.conf file directly. Here are a few examples of the rewrite rule in the Apache .htaccess file compared with the Cherokee .conf file (I have removed the vserver path from the Cherokee examples removed for clarity): # Example 1: In Apache .htaccess RewriteRule ^rss/ ([ a-zA-Z0-9_- ] + ) $ /rss.php?op = $1 # Example 1: In Cherokee cherokee.conf file regex = rss/ ([ ^/ ] + ) show = 0 substring = rss.php?op = $1 # Example 2: In Apache .htaccess RewriteRule ^ ([ a-zA-Z0-9_- ] + ) /ad/ ([ a-zA-Z0-9_- ] + ) /industry/ ([ a-zA-Z0-9_- ] + ) $ dashboard.php?language = $1 & module = ad & op = $2 & industry = $3 # Example 2: In Cherokee cherokee.conf file regex = ([ ^/ ] + ) /ad/ ([ ^/ ] + ) /industry/ ([ ^/ ] + ) show = 0 substring = dashboard.php?language = $1 & module = ad & op = $2 & industry = $3 Production Deployment Once I had Cherokee running perfectly in my development environment (aka my laptop), I proceeded to deploy to my production environment (aka Linode). Now, I could clone my Linode instance and deploy there and then update my DNS, but I was feeling brave and I was sure the deployment would go smoothly which it did. All I had to do was: Run server.bash to install cherokee Run a deply.bash script which (amongst other things) copies the cherokee.conf file to /etc/cherokee/ Stop Apache running /etc/intit.d/apache stop Start Cherokee /etc/init.d/cherokee start The site is now up and running on Cherokee. I hope you can join the Cherokee crowd one day too! "},{"title":"Installing Rails gem fails with &#8220;undefined method `spec&#8217; for nil:NilClass&#8221; error","url":"/2011/04/installing-rails-gem-fails-with-undefined-method-spec-for-nilnilclass-error/","date":"2011-04-15 00:00:00 +0700","categories":["bash scriping","rails appplication programming"],"body":"Today, while installing Rails 3, my computer suffered a power outage. This does not happen very often, in fact, this was actually due to the computer overheating! After pouring cold water over the keyboard, I restarted the computer and continuing the Rails 3 installation the process, but to my shock and horror, it failed with this message: ERROR: While executing gem ... ( NoMethodError ) undefined method ` spec ' for nil:NilClass I’ll be honest with you, I was kind of expecting problems to happen since the computer did abruptly shut off during an installation process – but now what do I do!? Well, after a little Googling, the problem was solved. I found out that it’s due to the gem install cache folder which (due to the power interruption) now contained corrupted / empty gem files. My solution was to clear this cache and run the install again, as follows: Find out the location of your cache using: gem env List all the gems in that cache, using, for example: ls -l /usr/lib/ruby/gems/1.8/cache/ Compare with successfully installed gems using gem list Remove the gems from the cache list that are missing from the gem list using the rm command Continue the installation, for example. sudo gem install -v=3.0.6 rails --no-rdoc --no-ri So now if you have a power outage or any other external failure during a gem installation process you don’t need to panic. Just clear the cache and continue where you left off before you were rudely interrupted! "},{"title":"Why is Bundler in Rails 3 so slow?","url":"/2011/04/why-is-bundler-in-rails-3-so-slow/","date":"2011-04-21 00:00:00 +0700","categories":["rails appplication programming","ruby server programming"],"body":"Actually, the title of this post is mis-leading a little so blame Google for bringing you here if this is not what you wanted! What I will do is tell you this – Bundler is really slow on my machine! It takes more than 40 minutes to install just a single gem! I have heard other complaints around the web-sphere but nowhere near as long as I have to wait (usually I find most people who have slow Bundler experience wait more than 3 minutes on average). OK, so I am not going to tell you why this is happening, but I will tell you a work around. Instead of running: bundle install …run it with the –local switch instead as follows: bundle install --local If you want to install a new Gem, lets say, mongo_mapper, then add to your application Gemfile: 1 gem 'mongo_mapper' …and here’s the important part, install it the old fashioned way, using gem install and then run bundle install --local 1 gem install mongo_mapper -- no - rdoc -- no - ri 2 bundle install -- local Note that I pass the –no-rdoc and –no-ri switch to gem install to make that a little (sometimes a lot) faster too. Now you can get back to building that app! Good luck! "},{"title":"Migrate a PHP application to Rails 3","url":"/2011/05/migrate-a-php-application-to-rails-3/","date":"2011-05-20 00:00:00 +0700","categories":["rails appplication programming"],"body":"I recently migrated a basic PHP website to Rails 3. The job went smoothly and this is my first Rails 3 app so I thought I would write about my experiences. Getting Started In this situation I would start directly at database level. Get the database ready so that you can create simple ActiveRecord classes without any rule bending. Here are some tips: Make sure all your table names are pluralized Make sure all your tables contain an id column as well as created_at and updated_at columns to adhere to Rails convention. Update the parent level tables first, create the models and quickly test them out in the console one-by-one Move on to child tables / associations and test these too in the console Note: Because you are building this ‘inside out’ so to speak (i.e. not using generators) you might come across issues like wanting to use reserved words for your models. in my case I needed to name a model Class (as in a taught class) but had to rename to Klass and use set_table_name as follows: 1 class Klass & lt ; ActiveRecord :: Base 2 set_table_name \"classes\" 3 end Date Formatting in Rails 3 Simply create a file under config/initializers/ directory called time_formats.rb and include formats that you might be needing throughout your application. in my case I wanted dates to be output as Full day, Day number, Month, Year e.g. Monday, 5th April 2011 1 Date :: DATE_FORMATS [ :schedule_date ] = \"%A %d %B %Y\" Rendering Collections with Partials in Rails 3 Something else that is quite useful is rendering partials with collections. By simply passing a collection to the render method, Rails will automatically render a partial with the singular name from the models view directory. For example, I have a list of courses under each category that I want to render, so by passing category.courses the view views/course/_course.html.erb is rendered for each course item in the collection. Marvelous! 1 & lt ; %= render category . courses %> Select drop down lists in Rails 3 Making select drop downs is amazingly clear and simple also. I made use of options_from_collection_for_select which is a very neat and powerful way to generate a set of options from an existing collection. Used in conjunction with select_tag renders our completed drop down list of options! 1 & lt ; %= select ( :enrollment , :class_id , options_from_collection_for_select ( @klasses , :id , :name )) %> Showing users error notifications in Rails 3 For some reason, the form builder helpers error_messages have been removed from Rails 3. I presume it is to allow the developer more control over rendering any validation errors. So if you want to output errors, use the following example to guide you: 1 & lt ; %= form_for(@enrollment) do |f| %> 2 &lt;% if @enrollment.errors.any? %> 3 <ol> 4 &lt;% @enrollment.errors.full_messages.each do |error_msg| %> 5 <li> 6 &lt;%= error_msg %> 7 </li> 8 & lt ; % end %> 9 10 </ol> 11 & lt ; % end %> SEO Friendly urls SEO Friendly urls are really easy to setup so put this in yer model and smoke it! The to_param method is used by the url helpers so by overriding in this way autmatically generates the url you choose with no further fuss! 1 def to_param 2 \" #{ id } - #{ title . parameterize } \" 3 end Sitemap XML file generation example in Rails 3 Sitemap XML file generation, I recommend: Sitemap Generator for Rails Note to get this working you need to add the following include to your rake task 1 include Rails . application . routes . url_helpers ...and also the following configuration change to your application.rb config 1 config . autoload_paths += %W( #{ config . root } /lib) Conclusion I am very happy that I moved my site www.cambodiahorizon.com to Rails 3 from PHP. It's much cleaner now, easier to maintain and fits in with my own personal desire to focus on building applications with Rails! "},{"title":"Learn Rails Backwards","url":"/2011/06/learn-rails-backwards/","date":"2011-06-13 00:00:00 +0700","categories":["rails appplication programming"],"body":"I sometimes run free Rails courses locally here. One of the things that I have learnt is that it seems to be best to teach Rails ‘backwards’. What do I mean by that? Well it depends on how you define ‘forwards’! If you practice TDD then a high level process would be to: Write your (failing) test in RSpec Write Rails code to pass the test This means knowing Ruby before you even start step one above, of course. However, when teaching Rails, I like to do things in reverse, namely: Learn how to write Rails code Learn how to write RSpec code Learn how to write Ruby code Now, of course it’s all Ruby, but the point is that the order when teaching / learning Rails is reversed compared with what is done in practice. Of course, this approach does have its good and bad points. Good Points It’s easier to jump straight into a framework with some simple examples and exercises to wow the students A lot of the time, Students can start building something cool right after the first lesson It’s especially good for new programmers as their interest in what is going on behind the scenes starts to seep through subconsciously. Bad Points It may be the beginning of a lifetime of bad habits (a.k.a “where are your tests!”). Following on from the above point, tests make for beautifully simple code – only do what you need and no more. If the student gets into the habit of writing tests after then the code will not be so simple. For some students, maybe the more experienced ones, it might be better off with the ‘forward’ approach: Learn Ruby, TDD / RSpec, Rails. What are your thoughts on this? "},{"title":"Using Value Objects in ActiveRecord","url":"/2011/07/using-value-objects-in-activerecord/","date":"2011-07-25 00:00:00 +0700","categories":["ruby server programming"],"body":"I wanted to find out more about using Value Objects in ActiveRecord. Value objects are conserned with the value of their attributes over other identifiers (as opposed to Entitiy Objects which are concerned mainly about a unique ID). I decided to create a stand alone Active Record project (i.e. without using Rails). This became a sort of mini exercise in itself. Structuring a standalone ActiveRecord project I like structure, so the setup I settled with is this: /db /models /config run.rb Inside config I placed two files a config.rb and database.yml file: config.rb 1 require 'rubygems' 2 require 'active_record' 3 require 'yaml' 4 require 'logger' 5 6 DBCONFIG = YAML :: load ( File . open ( 'config/database.yml' )) 7 ActiveRecord :: Base . establish_connection ( DBCONFIG ) 8 ActiveRecord :: Base . logger = Logger . new ( File . open ( 'db/database.log' , 'a' )) The database.yml file simply contains the details for my database connection. Inside my schema.rb I use ActiveRecord::Migration to “up” (create) and “down” (drop) my table in the database. All the details can be found in the sample project on Bitbucket . Structuring Value Objects The interesting part when concerning Value Objects are the two classes under the models directory person.rb and address.rb (shown below). Notice that since we want address data to be stored as a Value Object we do not inherit from ActiveRecord::Base. Instead, address is a plain Ruby object and is used in Person by using the composed_of class method. This allows for the attributes of the Value Object to be stored together in the database and composed_of allows us to use these together as one object. person.rb 1 require 'models/address' 2 3 class Person & lt ; ActiveRecord :: Base 4 composed_of :address , :mapping => [ %w(address_city city) , %w(address_county county) ] 5 end address.rb 1 class Address 2 attr_reader :city , :county 3 def initialize ( city , county ) 4 @city , @county = city , county 5 end 6 def == ( other_address ) 7 city == other_address . city & #038;&#038; county == other_address.county 8 end 9 end Using Value Objects So how to use Value Objects? Below is a sample application run.rb that uses the above models and creates four records in the database and searches for using a Value Object for records that have a particular address. The output from this applicaition is not surprisingly: Darren, Michael, proving that it is possible to pass in a new instance of a Value Object (in this case Address) and ActiveRecord will break that down and search for the address in the database for us. run.rb 1 require 'config/config' 2 require 'models/person' 3 4 p1 = Person . create ( :name => \"Darren\" , :address_city => \"Eastbourne\" , :address_county => \"Sussex\" ) 5 p2 = Person . create ( :name => \"Paul\" , :address_city => \"Nottingham\" , :address_county => \"Nottinghamshire\" ) 6 p3 = Person . create ( :name => \"Simon\" , :address_city => \"Dartford\" , :address_county => \"Kent\" ) 7 p4 = Person . create ( :name => \"Michael\" , :address_city => \"Eastbourne\" , :address_county => \"Sussex\" ) 8 9 people_in_eastbourne_sussex = Person . find_all_by_address ( Address . new ( \"Eastbourne\" , \"Sussex\" )) 10 puts people_in_eastbourne_sussex . map ( & #038;:name) The SQL that is generated for find_all_by_address is not surprising: SELECT \"people\" . * FROM \"people\" WHERE \"people\" . \"address_city\" = 'Eastbourne' AND \"people\" . \"address_county\" = 'Sussex' Another thing to note is that it is possible to reference either the Value Object via person.address or the attributes separately via person.address_city and person.address_county . "},{"title":"HTML5 Series: What makes HTML5 different?","url":"/2011/09/html5-series-what-makes-html5-different/","date":"2011-09-30 00:00:00 +0700","categories":["html5 programming","programming"],"body":"I decided during a holiday last week to read “Pro HTML5 Programming – Powerful APIs for Richer Internet Application Development” . It was my first real dive into HTML5 and I have to say I am very impressed. Sure, I had heard about all the goodness in HTML5 before reading this book but I had never, until now, actually experimented first hand. We are also developing a HTML5 application at Yoolk but as you can probably tell, I have never touched that code! Instead, I will probably build something myself just for fun or maybe as a project for Startup Weekend in some city! My overall feeling about HTML5 Forget other platforms (almost). HTML5 is the platform of choice I am very excited! HTML5 really feels like it is bridging the gap between native client apps build in .NET or GTK to pure web based (or should I say browser based?) applications. My feeling is “finally, pretty much any application that I dream up can be built for and run in a browser!”. Wow! This makes everything so much easier! No pondering should I use .NET, GTK, Flex or any other platform. There are still some exceptions like beefy enterprise applications which still require local install and setup but for 80% (based on the 80/20 principle , of course) HTML5 will do! Amazon Kindle Fire boots HTML5 adoption via the Silk Browser Now that Amazon have slammed the tablet market with their new Kindle Fire it opens up the potential for hundreds of thousands and, eventually millions, of new users who developers can target their HTML5 applications towards. The tablet device period opens up a whole new exciting opportunities for HTML5 applications and I can’t wait to get my hands on a Kindle Fire to try this out! The future direction of HTML5 seems really really bright Some of the future additions to HTML5 as mentioned in the Pro HTML5 Programming book are really cool. For example doing for audio what the canvas has done for images. Basically meaning that the tag gets an extremely rich API for editing, slicing, dicing and playing around with the audio pitch, volume and more! Can you imagine a full audio studio editing suite in the browser?! Another thing that looks really cool is the potential for the Canvas 3d context API. This can allow developers to produce 3d art, animation, patterns and effects! HTML5 Series Contents In this series I will go through many of the parts of the HTML5 API as well as attempting to try out some of the edge technology that is out there but not so wide spread. Over the next month or so, the following contents list will become links to actual pages in this blog! Getting started with the HTML5 Canvas API Getting started with the HTML5 Audio Video API Getting started with the HTML5 Geolocation API Getting started with the HTML5 Forms API Getting started with the HTML5 Web Storage API Getting started with the HTML5 Communications API Getting started with the HTML5 Web Sockets API Getting started with the HTML5 Offline Applications I hope that you enjoy the HTML5 series! "},{"title":"Getting started with the HTML5 Canvas API","url":"/2011/10/getting-started-with-the-html5-canvas-api/","date":"2011-10-08 00:00:00 +0700","categories":["html5 programming"],"body":"The Canvas API essentially allows the developer to draw on a web page using a JavaScript API. Now you might think, what is the point of this? If you want to show some artwork or animation why not create this first and then send it to the browser as a file? The main benefits of using the Canvas API The JavaScript to render the image / animation may be smaller than downloading a file The Canvas API allows for some cool extras like scaling, rotation and even pixel by pixel image access Being an API means that it is programmable, so you can build applications and games purely based on the Canvas API The Canvas API is potentially powerful enough that you could write an image editing suite that runs in the browser (no more Photoshop required!) The Canvas API does not require any browser plug-ins (bye, bye Falsh!) The Basics Eveytime you use the canvas API there are always some things you must do. The first thing is to use the DOM to get a reference to the Canvas object: var canvas = document . getElementById ( \"my-id\" ); Then we must get the 2D context of the Canvas object like so: (in the near future a 3D context will become more widely available) var context = canvas . getContext ( \"2d\" ); At this point we have access to the 2D context of our canvas object, so we can begin to call methods on this. The following example draws a line on the canvas: context . beginPath (); context . moveTo ( 10 , 10 ); context . lineTo ( 150 , 150 ); //Need to call stroke() function otherwise nothing will appear! context . stroke (); Drawing shapes You can do much more with the Canvas API than drawing lines. You can: Draw and fill shapes Draw lines Draw curves Add images Transform objects …and more! Here is an example of drawing circle using the arc function: context . arc ( 0 , 0 , 180 , 7 , 0.71 ); context . lineWidth = 5 ; context . stroke (); Here is a full example (JavaScript within the HTML Script tag) that uses the quadraticCurveTo function to draw a colorful arc! This example shows the HTML5 Canvas element as well as the full method to get the object and the context and draw the arcs. Notice that it is possible to set many properties on the context object like lineWidth. strokeStyle etc <canvas id= \"rainbow\" style= \"border: 1px solid;\" width= \"470\" height= \"400\" /> When run in a compatible browser the output should look like so: That's it for this very basic introduction! If you want to see the full power of the HTML5 Canvas API then check out some of the (really) cool examples below! Some cool examples Asteroids Game Starfield Landscapes Cloth Experiment Magnetic Experiment Some books and tutorials html5canvastutorials.com Canvas Deep Dive HTML5 Canvas book from Oreilly "},{"title":"Getting started with the HTML5 Audio Video API","url":"/2011/11/getting-started-with-the-html5-audio-video-api/","date":"2011-11-12 00:00:00 +0700","categories":["html5 programming"],"body":"The HTML5 Audio and Video API is a real game changer. For starters, think about how this affects the (once) mighty Flash. For years Flash was the most popular way to distribute audio and especially video via the web to a browser. Now that can change with HTML5. Even Adobe has announced that it is to stop developing Flash for mobile The Basics Getting started with the Audio and Video API is real easy (as is everything in HTML5, in fact!). At the very basic level all you need to do is declare an or tag in your html like so: <video src= \"media/some-video-file.mp4\" ></video> This is so basic that it does not even render the controls. In order to control video playback, the user has to use the context menu (which is not so easy to find for some). In order to render controls simply add the controls attribute like so: <video src= \"media/some-video-file.mp4\" controls ></video> Now we see the controls appear as sown below (notice that the first frame of the video is shown by default): Simple! So why does HTML5 have the option to not show the controls? Well the reason is to give the developer control over how the controls are rendered. Since JavaScript has an API to access the Audio and Video instances within the browser it is possible to create and style a “Play/Pause” button in anyway you like! Here is some example JavaScript code that demonstrates how to do this: 1 function playPauseMusic () { 2 var music = document . getElementById ( \"music\" ); 3 var playPauseBtn = document . getElementById ( \"playPauseBtn\" ); 4 if ( music . paused ) { 5 music . play (); 6 playPauseBtn . innerHTML = \"Pause the Music!\" ; 7 } 8 else { 9 music . pause (); 10 playPauseBtn . innerHTML = \"Play the Music!\" ; 11 } 12 } As you can see this is quite easy to do. Simply add an audio tag to the html page with id=”music” and a button tag with id=”playPauseBtn” and this JavaScript can be used to control the play / pause of your music! All it does is render a button but it works and demonstrates how easy it is to use the (in this case) Audio API (the Video API is very similar, by the way). Use the Source! So far I have only shown some simple examples. It is possible in HTML5 to specify different sources so that you can provide different file types for the media you want rendered. So in order to increase the chance that the browser supports the particular container or codec, make sure you save your media as different file types (e.g. ogg, mp4 etc) and use the source as in the following audio example: <audio> <source src= \"1.ogg\" > <source src= \"1.mp3\" > Some music by someone! </audio> Note the text ‘Some music by someone!’ just before the closing tag. This text will be rendered in non-supporting browsers so it provides a nice and easy way to gracefully fallback if that is the case. Autoplay attribute HTML5 Audio and Video API comes with the optional autoplay attribute. Use this with caution! There is nothing more annoying than a website that starts playing audio or video automatically on load! However, if you must, simply include the attribute in the or tag. Video API Basic Tip: Play Video on Mouseover One thing you might have seen on the web is the concept of playing a video in a thumbnail when the mouse hovers over the video image. This is surprisingly easy to do in HTML5, by the way! All you need to do is set the onmouseover and onmouseout events to call play() and pause() methods respectively, like so: <video id= \"movies\" onmouseover= \"this.play()\" onmouseout= \"this.pause()\" autobuffer= \"true\" > <!-- video source here --> </video> That’s it for this very basic introduction! If you want to learn more about the HTML5 Audio and Video API, check out some of the following resources: Some cool examples Basic Examples Video Events Example Some notes on HTML5 Video Codes support Some tutorials A great HTML5 Audio Video API tutorial by Mozilla Another great tutorial, this time from HTML5 Rocks! "},{"title":"Connecting a HTML5 application to a MongoDB instance via MongoLab REST API","url":"/2011/12/connecting-a-html5-application-to-a-mongodb-instance-via-mongolab-rest-api/","date":"2011-12-10 00:00:00 +0700","categories":["html5 programming","javascript client programming","nosql","rest programming"],"body":"Overview I needed a free, document based, online data store so that I could quickly build a HTML5 prototype. As an exercise, I quickly whipped up a simple application that can store basic contact details of people. Getting Started with MongoLab To get started with MongoLab is really easy. Simply create your account, your database and your first collection and your good to go. The only reference document you need for this exercise is the MongoLab REST API docs First things first: Adding data to your MongoDB using REST I am only going to show you the absolute basics here, so no security, validation, callbacks or fancy UI designs – just the basics! I built this by creating a simple HTML5 form and I used XMLHttpRequest API to send the data to MongoLab. Here are the code snippets, first the HTML: <form id= \"myform\" name= \"myform\" > <input id= \"name\" type= \"text\" name= \"name\" placeholder= \"Name\" ><br> <input id= \"email\" type= \"email\" name= \"email\" placeholder= \"Email\" ><br> <input type= \"button\" onclick= \"send(this.form);\" value= \"Save\" > </form> …and the JavaScript. Note the variable json which is not defined in this snippet but is basically a JSON representation of the form data. The easiest way to get your form data into JSON is to use JQuery (again, I am not including how to do that to keep us focused on how to interact with the MongoLab RESTful API). Note: you will need to replace YOUR-DATABASE, YOUR-COLLECTION and YOUR-API-KEY with your values. var xhr = new XMLHttpRequest (); xhr . open ( \"POST\" , \"https://api.mongolab.com/api/1/databases/YOUR-DATABASE/collections/YOUR-COLLECTION?apiKey=YOUR-API-KEY\" , true ); xhr . setRequestHeader ( \"Content-Type\" , \"application/json\" ); xhr . send ( json ); When you load your page you will be able to add a new record to the database. You can check that it has arrived at the database by logging into mongolab.com and viewing the collection. Next up: Display a collection of items It’s really easy to get data from the MongoLab API – a simple GET request will do, in fact! Apart from that, this step is really an exercise in using the JavaScript DOM (which I will not cover here). Here is the HTML – as you can see it is just a placeholder for the data. I am using UL but, of course, you can load the data into whatever placeholder you like. <ul id= \"result\" > </ul> Basic JavaScript snippet shown below. As you see just a simple GET request (and send null since there is no payload). To complete this you’ll want to add an onreadystatechange function to detect when the request.status is 200 and then call your DOM rendering function at that point. var request = new XMLHttpRequest (); request . open ( \"GET\" , \"https://api.mongolab.com/api/1/databases/YOUR-DATABASE/collections/YOUR-COLLECTION?apiKey=YOUR-API-KEY\" ); request . send ( null ); View a single document To render a single document in your HTML5 application, you’ll want to, again, use the GET request but this time passing in the document id in the query string and using that to pull the correct document from your MongoDB. The HTML snippet is the same as for the collection, so I’ll just show the JavaScript code below. Notice how to get the id from the query string. This exampled is hard coded, and thus assumes the id is first in the query string and is in the form “id=12345″. var queryString = window . top . location . search . substring ( 1 ); id = queryString . substring ( 3 , queryString . length ); var request = new XMLHttpRequest (); request . open ( \"GET\" , \"https://api.mongolab.com/api/1/databases/YOUR-DATABASE/collections/YOUR-COLLECTION/\" + id + \"?apiKey=YOUR-API-KEY\" ); request . send ( null ); Edit a document Since this is a RESTful API, to update a document we need to use a PUT verb. The HTML snippet is almost the same as the adding data example, so just the JavaScript snippet here. Notice the getId() function call which simply gets the id from the query string. Obviously this is not a production ready or secure way of getting the id for a document we want to update. var xhr = new XMLHttpRequest (); xhr . open ( \"PUT\" , \"https://api.mongolab.com/api/1/databases/YOUR-DATABASE/collections/YOUR-COLLECTION/\" + getId () + \"?apiKey=YOUR-API-KEY\" , true ); xhr . setRequestHeader ( \"Content-Type\" , \"application/json\" ); xhr . send ( json ); Delete a document Finally, we want to be able to delete a document. This is really easy as shown. Notice you need to send null like before because there is no payload. var request = new XMLHttpRequest (); request . open ( \"DELETE\" , \"https://api.mongolab.com/api/1/databases/YOUR-DATABASE/collections/YOUR-COLLECTION/\" + id + \"?apiKey=YOUR-API-KEY\" ); request . send ( null ); Conclusion Thankfully, with the excellent service provided by MongoLab.com developers are able to build nosql document database driven applications in no time and at no cost! "},{"title":"Getting started with the HTML5 Geolocation API","url":"/2011/12/getting-started-with-the-html5-geolocation-api/","date":"2011-12-22 00:00:00 +0700","categories":["html5 programming"],"body":"Geolocation presents major opportunities for mobile application development. It’s an exciting way to work with location based applications and services and really give the user something that is of great value to them. By using Geolocation, developers can find out the location of the user, can give the user directions to other places, can tell the user where the nearest XYZ place might be, can tell the user how far they have moved in a given timeframe and more, and more… The Basics When using the Geolocation API it is important to know the data that we are working with. With Geolocation we work with Latitude and Longitude values. These represent the following: Latitude : The distance North or South of the equator Longitude : The distance East or West of Greenwich, England Usually, these values come from the mobile device GPS or, if the GPS is not available then the co-ordinates can be inferred from the IP address, Wi-Fi or even entered directly by the user. Privacy Since we are dealing with sensitive information, there are strict privacy requirements that must be followed when working with the Geolocation API. Fortunately, privacy user confirmation is triggered in all browses (that support Geolocation) automatically on any API call. You may have seen the message in some applications asking if it is ok that the application uses your location. This is Geolocation API security kicking in! It happens only once per domain so once you accept you will not need to accept every time you perform a Geolocation based action on that site. Getting the co-ordinate data To get the current position of the user, simply execute this function. Note that updateLocation is a callback function to handle the result. navigator . geolocation . getCurrentPosition ( updateLocation ) The position object passed to the updateLocation() function contains the following data attributes: latitude longitude accuracy Here is an example of how the updateLocation function might appear: function updateLocation ( position ) { var latitude = position . coords . latitude ; var longitude = position . coords . longitude ; var accuracy = position . coords . accuracy ; var timestamp = position . timestamp ; document . getElementById ( \"latitude\" ). innerHTML = latitude ; document . getElementById ( \"longitude\" ). innerHTML = longitude ; document . getElementById ( \"accuracy\" ). innerHTML = accuracy ; document . getElementById ( \"timestamp\" ). innerHTML = timestamp ; } Handling Errors It’s important to handle errors in Geolocation API calls because there is so much potential for things to go wrong: network error, GPS issues, privacy / security issues, timeout etc. We can handle errors using another callback function that takes an error code and deals with that code accordingly. What we need to do first is include that callback function in the call to the getCurrentPosition Geolocation function: navigator . geolocation . getCurrentPosition ( updateLocation , handleError ) Then our handleError callback function might look like this (notice the 4 possible error codes): function handleError ( error ) { switch ( error . code ) { case 0 : updateStatus ( \"There was an error while retrieving your location: \" + error . message ); break ; case 1 : updateStatus ( \"The user prevented this page from retrieving a location.\" ); break ; case 2 : updateStatus ( \"The browser was unable to determine your location: \" + error . message ); break ; case 3 : updateStatus ( \"The browser timed out before retrieving the location.\" ); break ; } } Repeated requests to keep track of users position In the example above, getCurrentPosition() *makes one call to get the users location. However, there maybe situations where you will want to track the users *change in position over time. In this case it is better to use watchPosition() . Fortunately, it’s really simple to change to use watchPosition, here is an example so you can see how similar it is. Note we can set the watchId variable so that we can tell the API to ‘stop watching ‘ by making a call to clearWatch function at a later time: var watchId = navigator . geolocation . watchPosition ( updateLocation , handleError ); //Do something and sometime later we can stop watching by passing the watchId to clearWatch(): navigator . geolocation . clearWatch ( watchId ); Conclusion While this was a very basic introduction to the HTML5 Geolocation API, it is enough for developers to get started building something useful! Remember that you can also use Google Maps API to integrate mapping data into your application too! Some cool examples Basic Example Demo’s difference with IP and Geolocation API based calls Some tutorials Basic Tutorial Introduction Tutorial More detailed tutorial "},{"title":"Ruby blocks, Procs and lambdas","url":"/2012/01/ruby-blocks-procs-and-lambdas/","date":"2012-01-05 00:00:00 +0700","categories":["ruby server programming"],"body":"Preamble One of my favorite parts of the Ruby Programming language is being able to pass code to a method (usually an iterator) and have that code executed dynamically. In Ruby there are several ways to do this using: blocks, Procs and lamdbas. While these are all very similar, there are some very subtle differences. First let me explain blocks. Blocks Blocks are code that can be implicitly passed to a method in Ruby. Any method can in fact be passed a block of code even if the method does not explicitly expect it. In the following example, the method test_blocks does not explicitly expect or do anything with a block, however, if called with a block the code will run printing “in test_blocks” 1 def test_blocks 2 puts \"in test_blocks\" 3 end 4 5 test_blocks { puts \"in the block\" } 6 7 # Output: 8 # in test_blocks By simply adding the yield keyword to the method test_blocks the block will then be run causing the output to change: 1 def test_blocks 2 puts \"in test_blocks\" 3 yield 4 end 5 6 test_blocks { puts \"in the block\" } 7 8 # Output: 9 # in test_blocks 10 # in the block Procs It is possible to get a handle on the block by explicitly putting the block as a parameter in the method definition. Note that this must be the last parameter and must start with an ampersand: 1 def test_blocks ( & #038;block) 2 puts block . class 3 end 4 5 test_blocks { puts \"in the block\" } 6 # Output: 7 # Proc Notice that the class is Proc and so the only difference between a block and a Proc is that a Proc can be passed around as an explicit variable. Here is another example where we create a Proc object first and pass that into another method that expects a Proc: 1 def test_blocks ( some_proc ) 2 puts some_proc . call 3 end 4 5 some_new_proc = Proc . new { puts \"in the Proc !\" } 6 test_blocks ( some_new_proc ) 7 8 # Output: 9 # in the Proc ! The main reason for having both block and Proc is: If we want to write Ruby like syntax when passing code to a method, and we only want to pass a single Proc we use blocks . This is very common pattern in Ruby code – especially with iterators. If we want to save a piece of code in a variable for reuse then use an explicit Proc object If we want to pass multiple blocks of code to a method and invoke them, we also must use an explicit Proc object Lambda Lambda is very similar to a Proc. Let’s first start off with an example: 1 def test_lambda ( some_lambda ) 2 some_lambda . call 3 end 4 5 some_new_lambda = lambda { puts \"in the lambda\" } 6 test_lambda ( some_new_lambda ) 7 8 # Output: 9 # in the Proc ! As you can see this looks exactly like the Proc example above, so what is the difference? Well the answer is in the way Ruby handles Procs and Lamdbas. There are, in fact, two differences: Lamdbas check the number of parameters passed into the call, Procs do not check the number of parameters Lamdbas return from the executing block but not from the lexically surrounding method call when they encounter a return keyword (in other words lambda behaves like calling a method which then calls return, whereas a Proc will return from the calling method too Here are a couple of examples to illustrate this. Firstly, the parameter handling differences: 1 def test_parameter_handling ( code ) 2 code . call ( 1 , 2 ) 3 end 4 5 l = lambda { | a , b , c | puts \" #{ a } is a #{ a . class } , #{ b } is a #{ b . class } , #{ c } is a #{ c . class } \" } 6 p = Proc . new { | a , b , c | puts \" #{ a } is a #{ a . class } , #{ b } is a #{ b . class } , #{ c } is a #{ c . class } \" } 7 8 test_parameter_handling ( p ) 9 test_parameter_handling ( l ) 10 11 # Output: 12 # 1 is a Fixnum, 2 is a Fixnum, is a NilClass 13 # ArgumentError: wrong number of arguments (2 for 3) As you can see from the output of the example, the Proc does not care if we only call with two parameters when it’s possible to use three parameters – it simply sets all unused parameters to nil . However, when passing a lambda to the method, Ruby throws an ArgumentError instead. So if you want your Proc (a lamdba is an instance of Proc, by the way) to be called with a specific set of parameters – no more and no less – then use lamdba . Here is an example that demonstrates the differences how Proc and lamdba behave when they encounter a return statement: 1 def return_using_proc 2 Proc . new { return \"Hi from proc!\" } . call 3 puts \"end of proc call\" 4 end 5 6 def return_using_lambda 7 lambda { return \"Hi from lambda!\" } . call 8 puts \"end of lambda call\" 9 end 10 11 puts return_using_proc 12 puts 13 14 # Output 15 # Hi from proc! 16 # end of lambda call When return_using_proc is called the method stops processing as soon as it encounters the return statement and we see “Hi from proc!” output. When return_using_lambda is called the return is encountered but does not cause the method to return and instead the method continues to run so we see “end of lambda call” output. Here is another example to demonstrate this in a slightly different way: 1 def test_returns 2 puts \"top\" 3 4 pr = Proc . new { return } 5 pr . call 6 7 puts \"bottom\" 8 end 9 10 puts \"before call\" 11 test_returns 12 puts \"after call\" 13 14 # Output 15 # before call 16 # top 17 # after call In this example, we see ‘before call’ output followed by ‘top’ as we would expect but then we see ‘after call’ – note there is no output for our puts ‘bottom’ statement. This is because the call to the Proc object (pr.call) caused the calling method to return since the Proc contained a return statement. If we replace the Proc with a lambda then we would see ‘bottom’ output as well since the call to the lambda simply returns back to the method which called it. The main reason for this diffence is because Procs are more like blocks of code that can be ‘dropped in’ and executed whereas lamdbas behave more like methods. It is also for this reason why lambdas are more strict with parameter checking – just as methods are. Ruby Closures It’s important to note that blocks, Procs and lambdas are all closures . Basically this means that they hold the values of any variables that were around when they were created. This is a very powerful feature of Ruby because if we wrap creating a block of code in a method call we can dynamically create different behavior. For example: 1 def create_multiplier ( m ) 2 lambda { | val | val * m } 3 end 4 5 two_times = create_multiplier ( 2 ) 6 three_times = create_multiplier ( 3 ) 7 8 puts two_times . call ( 1 ) 9 puts three_times . call ( 1 ) 10 11 # Output 12 # 2 13 # 3 Conclusion In conclusion we are dealing with blocks of code that can be passed to method calls and executed within that method. When the block of code is executed in the method the state of any variables at the time of creation of that block of code are used. This means that blocks, Procs and lamdbas are all closures in Ruby. It’s also interesting to conclude that: Lambdas have strict parameter checking and diminutive returns. Procs have no parameter checking and strong returns. Blocks and lambdas are essentially just anonymous Procs Some tutorials Understanding Ruby Blocks, Procs and Lambdas The Ruby Object Model and Metaprogramming – Episode 3 "},{"title":"Preview a local image using Backbone and HTML5 JavaScript","url":"/2012/01/preview-a-local-image-using-backbone-and-html5-javascript/","date":"2012-01-18 00:00:00 +0700","categories":["html5 programming","javascript client programming"],"body":"HTML5 + Backbone.js make a great partnership! In this blog post, I am going to show how to preview an image in the browser without having to touch the server. I’ll also use Backbone.js to demonstrate how developers can update a Backbone model with image data and refresh the view to in order to display a preview of the image. Backbone Templates Backbone Templates allow developers to build re-usable layouts that can be instansiated within Backbone Views. For this example application, we will need two templates: one for the preview and one for selecting (and later saving) a new image file. Here is the HTML: Backbone Views Backbone Views have the responsibility of initializing and rendering the templates into the DOM so that they are visible on the screen. They can also include data from the model. In the templates above, only one requires data from the model; preview-image-template . This is also, in fact, the most basic view in this example, so let's look at that first: I won't go into too much detail about what is happening here with the Backbone View but instead I want to focus on the task at hand; to load a preview of the selected image file. The main code to take notice here is the following call to bind in the initialize function: this . model . bind ( 'change' , this . render ); This binds the change event on the model to call the render method on this view. The result will be that whenever any attribute on the model changes, the render method on this view will be called, resulting in the display being updated. But with what? Let's continue to build the application with the SelectImageView shown below: In this View I am binding to some DOM events which I am then passing onto View dispatch functions. I prepended these functions with dispatch because in a Production application environment developers would usually trigger custom events which would then be bound by Backbone Models or Views. In this case, I am cheating a little for simplicity and clarity and simply calling the setFromFile function directly on the model. This model, by the way, is exactly the same instance of the model that is bound to the PreviewImageView (see the bootstrap.js file description below to see how developers can set the model attribute on a Backbone View). Backbone Models This is by far the most interesting part of the application! There is only one model and so this is where we need to define our setFromFile function used in the view earlier. This function will use the new HTML5 FileReader object to read the data from the selected file and update the data attribute on the model with the raw data from the file. As you would have guessed by now, this fires a change event on the model which in turn causes the PreviewImageView to call render and we should see the new image appear on the screen! Backbone Router and bootstrap.js For completeness, here is the code for this applications Backbone Router and the bootstrap.js code that kicks everything off. The router does nothing unusual but bootstrap.js deserves a little explanation. I want the application to load in a state where we see a default image ‘preview.png’. This can be achived by setting the model with default attributes of filename = ‘preview.png’ and data = ‘img/preview.png’. This can also be done directly with the model definition too using defaults. Next I create both views and set the model property on each using the same model. As mentioned above, in Production, we would only set the model property on the PreviewImageView and use events to notify of any changes. Finally we create the Backbone Router and start the application! Here is the code for the Backbone Router and bootstrap.js: Some cool tutorials on the subject HTML5 Rocks Backbone Tutorials "},{"title":"Save an image file directly to S3 from a web browser using HTML5 and Backbone.js","url":"/2012/01/save-an-image-file-directly-to-s3-from-a-web-browser-using-html5-and-backbone-js/","date":"2012-01-19 00:00:00 +0700","categories":["amazon ec2 s3 cloud","html5 programming","javascript client programming"],"body":"This post continues from the previous post where I show how to preview a local image using Backbone and HTML5 JavaScript . In this post we will actually upload this file directly to an S3 bucket! This post will focus only on that part. Overview The way to avoid any cross-site-scripting restrictions when uploading to S3 is essentially to do two things: Use a form to POST the data to S3 Include in that form a Policy and Signature based on the content + your AWS keys The Form Since I am using Backbone.js in this example and since I want to show a preview of the image before uploading it, I realized that I needed to split the form into two templates. The reason was because I needed to be able to refresh the file meta data – that is essentially the key , policy and signature required by the AWS S3 API. Here are the two templates: The Image Meta Template <script type= \"text/template\" id= \"image-meta-template\" > < input type = \"hidden\" name = \"key\" value = '<%= key %>' /> < input type = \"hidden\" name = \"acl\" value = '<%= acl %>' /> < input type = \"hidden\" name = \"Content-Type\" value = '<%= contentType %>' /> < input type = \"hidden\" name = \"AWSAccessKeyId\" value = '<%= AWSAccessKeyId %>' /> < input type = \"hidden\" name = \"success_action_redirect\" value = '<%= successActionRedirect %>' /> < input type = \"hidden\" name = \"x-amz-meta-filename\" value = '<%=filename %>' /> < input type = \"hidden\" name = \"Policy\" value = '<%= POLICY %>' /> < input type = \"hidden\" name = \"Signature\" value = '<%= SIGNATURE %>' /> </script> The Image File Template <script type= \"text/template\" id= \"image-file-template\" > < form id = \"formBlob\" action = \"<%= bucket %>.s3.amazonaws.com\" method = \"post\" enctype = \"multipart/form-data\" > < input id = \"myImage\" type = \"file\" name = \"file\" /> < input id = \"btnSave\" type = \"submit\" value = \"Save Image\" /> < /form> </script> The data is passed to these templates from the Backbone View. However, when we select a new file we need to first update the preview (this template is discussed in the previous post ) and then update the image meta template only (not the image file template because that is already correct). So this is the reason I have two templates here. The interesting attributes of the image meta template are: key , POLICY and SIGNATURE which are all required by the AWS S3 API. The key is the simplest of the three, so let’s start with that. The Key The key is simply the bucket key we need to use in order to save (and later retrieve) the image back. In this example, I have the key made up of two attributes on the Backbone Model: folder and filename. The folder is set during the creation of the model in the bootstrap.js file and the filename is set when the user selects a file using the myImage file input. Right after the file has been sucessfully loaded into the preview, I call a function “updatePoilcy” which first sets the key attribute on the Model like so: updatePolicy : function (){ var key = this . get ( 'folder' ) + this . get ( 'filename' ); this . set ({ key : key }); //more code later... } The Policy The Policy is a JSON document that is then converted to Base64 encoding and set as a property on the Model. The code to do this is also in the updatePolicy function (hence the name!). Note that we are getting the data directly from the Model to build this Policy and that it must (and will , thanks to Backbone binding!) match with the values in the form template. Here is that code: updatePolicy : function (){ //code to set the key attribute on the model POLICY_JSON = { \"expiration\" : \"2012-12-01T12:00:00.000Z\" , \"conditions\" : [ [ \"eq\" , \"$bucket\" , this . get ( 'bucket' )], [ \"starts-with\" , \"$key\" , this . get ( 'key' )], { \"acl\" : this . get ( 'acl' )}, { \"success_action_redirect\" : this . get ( 'successActionRedirect' )}, { \"x-amz-meta-filename\" : this . get ( 'filename' )}, [ \"starts-with\" , \"$Content-Type\" , this . get ( 'contentType' )] ] }; var policyBase64 = Base64 . encode ( JSON . stringify ( POLICY_JSON )); //Set the Policy on the Model so that it is then sent with the Form payload this . set ({ POLICY : policyBase64 }); //more code later... } The Signature The final part of this puzzle is the Signature. This is generated using the Policy and your AWS Secret key and is encrypted using SHA1 like so: updatePolicy : function (){ //previous code (shown above) var secret = this . get ( 'AWSSecretKeyId' ); var policyBase64 = Base64 . encode ( JSON . stringify ( POLICY_JSON )); var signature = b64_hmac_sha1 ( secret , policyBase64 ); this . set ({ SIGNATURE : signature }); } The full updatePolicy function looks like this. Note that this function is called on the Model initialize and on any data changes (i.e. when the user selects a new image). updatePolicy : function (){ var key = this . get ( 'folder' ) + this . get ( 'filename' ); this . set ({ key : key }); POLICY_JSON = { \"expiration\" : \"2012-12-01T12:00:00.000Z\" , \"conditions\" : [ [ \"eq\" , \"$bucket\" , this . get ( 'bucket' )], [ \"starts-with\" , \"$key\" , this . get ( 'key' )], { \"acl\" : this . get ( 'acl' )}, { \"success_action_redirect\" : this . get ( 'successActionRedirect' )}, { \"x-amz-meta-filename\" : this . get ( 'filename' )}, [ \"starts-with\" , \"$Content-Type\" , this . get ( 'contentType' )] ] }; var secret = this . get ( 'AWSSecretKeyId' ); var policyBase64 = Base64 . encode ( JSON . stringify ( POLICY_JSON )); var signature = b64_hmac_sha1 ( secret , policyBase64 ); this . set ({ POLICY : policyBase64 }); this . set ({ SIGNATURE : signature }); } Success Action Redirect The successActionRedirect attribute on the model is worth a mention. In this example, I set it to the same page that sent the POST request. If you only want to send data to S3 this is fine, but what if you want to sent some of the meta data to another separate API? There are several ways to do this but one way would be to set this attribute to a service that sends the bucket and key to a backend queue. That queue can be periodically ‘popped’ via a cron job that makes a GET request to S3 to retrieve all the meta data saved there for that image file. This data can then be saved to a separate API at this point. This gives the developer full control over where the file meta data is transformed and stored. Another thing to note is the attribute x-amz-meta-filename . This is actually a custom attribute and the developer can set any number of these with the POST request. They must start with “x-amz-meta-” but that’s about it! So you could have “x-amz-meta-width” & “x-amz-meta-height” which could be calulated using a HTML5 Canvas set and set as properties on the Model. As long as the Policy and the Form contain these same attributes, they will all be saved along with the image when the form is POST to S3. Very handy! Conclusion This is a basic example to show how to upload a file directly to S3 from the browser. There are still plenty of unanswered questions such as being able to upload multiple files or upload a file using AJAX and then post some meta data to a separate API (all directly from the browser). These issues can be the subjects of future posts. Code The code for this example is available here To run the example you need to serve the folder from a HTTP server. I usually use Python SimpleHTTPServer for basic examples like this one. Thus, by default, if you CD into the example directory and run python -m SimpleHTTPServer then the application will be available at http://localhost:8000 . You also need to create an S3 account and update YOUR-BUCKET , YOUR-ACCESS-KEY and YOUR-SECRET-KEY in bootstrap.js. Oh! and it will only work in the latest WebKit based browsers. I tested it in Chrome 16.0.912.63 beta. Good luck! Tutorials POST Example from AWS Upload S3 files directly with AJAX Another example from AWS "},{"title":"Basic multiple image gallery upload HTML5 and Backbone application","url":"/2012/01/basic-multiple-image-gallery-upload-html5-and-backbone-application/","date":"2012-01-23 00:00:00 +0700","categories":["html5 programming","javascript client programming","ruby server programming"],"body":"This is really a very basic example, with the main aim to get anyone started with a simple image upload and persist application using HTML5 and Backbone. This post is essentially a continuation of a previous post where I explained how to preview a local image using HTML5 and Backbone . The main expansion here is that I am adding the capability to: Preview multiple images using a multiple file input Add all the images to a Backbone collection and persist that collection to a server Backbone Templates I’ll start with the templates again. In this application I have three templates: one for selecting multiple images, one for displaying a single image preview and a container template for the header text and a placeholder for the collection. The select multiple images template looks as follows: <script type= \"text/template\" id= \"gallery-selection-template\" > < input id = \"myGallery\" type = \"file\" name = \"file\" multiple />< br /> < button id = \"saveGallery\" > Save Gallery < /button> </script> The single image preview template is as follows: <script type= \"text/template\" id= \"image-template\" > < img class = \"thumb\" src = '<%= data %>' title = '<%= filename %>' /> </script> Finally, the container is the most basic template, since it’s just a title and a placeholder for the collection: <script type= \"text/template\" id= \"gallery-template\" > < h2 > Image Gallery < /h2> < output id = \"thumbnails\" /> </script> There is nothing really particullarly special to mention with these templates, so let’s move on. Backbone Views I only want to focus on the enhancements I have made which is being able to preview a collection of images and save them to the server via a Backbone collection. Therefore, I will focus on one view here: GalleryView . Actually, this is a standard way of building a view that renders a collection of sub-views in it’s render method. Here I get a handle on the template gallery-template and append the rendered out ImageView for each Image in the Backbone collection for the Image Gallery: 1 window . GalleryView = Backbone . View . extend ({ 2 template : _ . template ( $ ( \"#gallery-template\" ). html ()), 3 4 initialize : function () { 5 _ . bindAll ( this , 'render' ); 6 this . collection . bind ( 'add' , this . render ); 7 }, 8 9 render : function (){ 10 var $images , 11 collection = this . collection ; 12 13 $ ( this . el ). html ( this . template ({})); 14 $images = this . $ ( \"#thumbnails\" ); 15 this . collection . each ( function ( image ) { 16 var view = new ImageView ({ 17 model : image , 18 collection : collection 19 }); 20 $images . append ( view . render (). el ); 21 }); 22 23 return this ; 24 } 25 }); Backbone Collections The Gallery collection is responsible for loading the image data into Backbone Image models and adding them to the collection. There is a function setFromFiles which expects a FileList object passed to it from the multiple file input. Here is the collection code: window . Gallery = Backbone . Collection . extend ({ model : Image , url : \"/images\" , setFromFiles : function ( files ) { this . reset (); self = this ; for ( var i = 0 , f ; f = files [ i ]; i ++ ) { var reader = new FileReader (); reader . onload = ( function ( theFile , theId ) { return function ( e ) { image = new window . Image (); image . set ({ id : theId }) image . set ({ filename : theFile . name }); image . set ({ data : e . target . result }); self . add ( image ); }; })( f ); reader . readAsDataURL ( f , i ); } } }); The code that calls the setFromFiles method is actually in the ImageSelectionView (that renders the gallery-selection-template ) and is called when a change event occurs on the multiple file input. The function looks like this: dispatchUpdateGalleryPreview : function ( e ) { this . collection . setFromFiles ( e . target . files ); } The e.target.files *is the *FileList object that the setFromFiles function expects. Note I have prepended the function name with dispatch because it’s good practice in production code to have the application event driven and therefore this function would actually dispatch a custom event instead of calling the function directly on the collection. Backbone Sync Finally, we arrive at the persistence part! On the client side we use a little more Backbone. In this case, I simply want to save the entire collection, so I will use Backbone.Sync as follows: Backbone . sync ( \"create\" , this . collection ); This will cause Backbone to perform a POST request to the server to the location: ‘/images’ which is basically the url set in the Gallery collection earlier. One the server a Ruby file is waiting to process the response. In this example, I am using Sinatra server. Of course, developers are free to use any server technology they want! For completeness, here is the server side Ruby code: 1 post \"/images\" do 2 payload = JSON . parse ( request . body . read . to_s ) 3 4 payload . each do | image | 5 data = image [ \"data\" ] 6 i = data . index ( 'base64' ) + 7 7 filedata = data . slice ( i , data . length ) 8 9 File . open ( image [ \"filename\" ] , 'wb' ) do | f | 10 f . write ( Base64 . decode64 ( filedata )) 11 end 12 end 13 14 end Conclusion This is a very basic application which demonstrates selecting, previewing and saving multiple image files to a Ruby Sinatra endpoint using Backbone and HTML5 on the client. There is no functionality to manage a collection of images, for example, edit or delete. However, I feel this example is enough to get started to build a fuller Image Gallery application! Many features could be implemented in this application including using the HTML5 Canvas to automatically resize images or allow the user to alter the images directly before saving. On the server side, the Ruby script could actually push the images to S3 instead of saving to the file system, for example. Code The code for this example is here . Tutorials Great tutorial using FileReader by HTML5 Rocks "},{"title":"Ruby Generators","url":"/2012/02/ruby-generators/","date":"2012-02-16 00:00:00 +0700","categories":["ruby server programming"],"body":"Simple definition of Generators (Note there is a full definition available on Wikipedia ) I think the main thing to take away from the Wikipedia definition is that a generator is something* that looks like a function and behaves like an iterator*. What this means is that we can create a generator and call it, however many times we want, and it will return the next value in the iteration. It means (as iterators do) that we can have more control over what happens before and after each iteration. It also means that developers get the additional power to have complete control over what the iterator actually returns (for example a sequence of Prime Numbers). Another benefit is that they require less memory than say iterating over a large array that’s already explicitly defined. So what does a Generator look like in Ruby? Since Generators are like Iterators a good example to work with would be to write a program that generates a subset of Prime Numbers or Fibonacci Numbers – so this is what we will do in Ruby using a number of different approaches. In Ruby, Enumerators are a form of Generators so here is a sligtly modified example (to use Ruby 1.9.2 Enumerator class instead of the Ruby 1.8.7 Generator) from Anthony Lewis’s blog. This example generates a set of Prime Numbers: 1 g = Enumerator . new do | g | 2 n = 2 3 p = [] 4 5 while true 6 if p . all? { | f | n % f != 0 } 7 g . yield n 8 p & lt ; & lt ; n 9 end 10 n += 1 11 end 12 end 13 14 p g . take ( 10 ) #=> [2, 3, 5, 7, 11, 13, 17, 19, 23, 29] Here is another example to generate Fibonacci Numbers that uses the Enumerator class, this is a slightly modified example taken from the Ruby docs . The only change I made here is that I am calling yield instead of using the alias << . You could think of the call to yield or << as like sending out the value back to the caller (or at least that’s the way I think of it when I see a yield call in a Generator!) 1 fib = Enumerator . new do | y | 2 a = b = 1 3 loop do 4 y . yield a 5 a , b = b , a + b 6 end 7 end 8 9 p fib . take ( 10 ) # => [1, 1, 2, 3, 5, 8, 13, 21, 34, 55] Little more advanced example If you want to build your own Generator, using lambdas, check out this example from Victor Moroz 1 fib = 2 lambda { 3 a , b = 0 , 1 4 lambda { begin a ensure a , b = b , a + b end } 5 } [] 6 7 puts ( 0 . . 19 ) . map { fib [] } . inspect Full definitions of Generators (Full definition from Wikipedia ): “A generator is a special routine that can be used to control the iteration behaviour of a loop. A generator is very similar to a function that returns an array, in that a generator has parameters, can be called, and generates a sequence of values. However, instead of building an array containing all the values and returning them all at once, a generator yields the values one at a time, which requires less memory and allows the caller to get started processing the first few values immediately. In short, a generator looks like a function but behaves like an iterator .” "},{"title":"Create reusable UI components and widgets using Backbone.js","url":"/2012/03/create-reusable-ui-components-and-widgets-using-backbone-js/","date":"2012-03-30 00:00:00 +0700","categories":["javascript client programming"],"body":"I was building an application in Backbone.js recently and I wanted to experiment with an idea I had to build UI components using Backbone.js. I was given the opportunity when we required a notification element to be displayed on the page. It is possible, to do this in CSS or say using Twitter Bootstrap, however, I wanted to turn this into a basic Backbone.js UI component. In order to do this we need to think about how the component can be reused in different applications; i.e. not tied directly to any domain specific markup, schema or business logic in the application that is being built. Now, you would probably want to create a separate repository for this project since, after all, it is not strictly meant to live inside an application (unless, of course, your not interested to use the component in any other project other than the one your working on). The Template The template has a single placeholder message which can be set in the instance of the model that is used in the application. Note, from the line class=”alert fade in” that this particular component will work best with Twitter Bootstrap . <script id= \"notification-template\" type= \"text/template\" > < div > < div id = \"notification\" class = \"alert fade in\" > < a class = \"close\" data - dismiss = \"alert\" href = \"#\" > × < /a> < h3 ><%= message %>< /h3> < /div> < /div> </script> The Model The Notification model has two defaults set: message and status (which is another model). Notice the namespace com.tweetegy.ui used to keep the objects separate from the application. com . tweetegy . ui . NotificationModel = Backbone . Model . extend ({ defaults : { message : \"Loading...\" , status : com . tweetegy . ui . NotificationStatusMessage } }); Notification Status Types The Notification Status Types Model acts as an enum holding strings to represent the status of each notification that is shown. This is used to control the style of the notification. com . tweetegy . ui . NotificationStatusMessage = \"NotificatonStatusMessage\" ; com . tweetegy . ui . NotificationStatusError = \"NotificatonStatusError\" ; Backbone View This is the object that wraps everything up and allows us to reuse this as a component (as we will see later). This is just a standard Backbone View, with an added helper method updateStyleFromStatus which changes the style depending on the status of the Notification and closeNotifcation that hides it. The other important thing to note is the line this.model.bind(“change”, this.render); which is where we use the ‘magic’ of Backbone data binding to render our notification anytime the message or status of the applications notification model changes. It means that all we need to do in our application is set either the message or the status and the view will render itself. com . tweetegy . ui . NotificationView = Backbone . View . extend ({ className : \"well form-inline\" , events : { \"click .close\" : \"closeNotification\" }, initialize : function () { _ . bindAll ( this , 'render' , 'updateStyleFromStatus' ); this . template = _ . template ( $ ( '#notification-template' ). html ()) this . model . bind ( \"change\" , this . render ); }, render : function () { var self = this ; $ ( this . el ). html ( this . template ( this . model . toJSON ())); this . updateStyleFromStatus (); $ ( this . el ). find ( \"#notification\" ). show (); return this ; }, closeNotification : function () { $ ( this . el ). find ( \"#notification\" ). hide (); }, updateStyleFromStatus : function () { var newClass = \"alert fade in\" ; if ( this . model . get ( 'status' ) == window . NotificationStatusError ) { newClass = \"alert alert-block alert-error fade in\" } $ ( this . el ). find ( \"#notification\" ). removeClass (). addClass ( newClass ); } }); Using the component I used it in an application called “Front Desk” and initialized it in the bootstrap.js file during application initialziation: com . tweetegy . FrontDesk . notification = new com . tweetegy . ui . NotificationModel (); this . _notificationView = new com . tweetegy . ui . NotificationView ({ model : y . f . FrontDesk . notification }); $ ( 'div#notification' ). append ( this . _notificationView . render (). el ); As mentioned above, in the Backbone View we bind to the models change event to render the view which means all we need to do is change the data in our applications NotificationModel instance (in this case com.tweetegy.FrontDesk.notification ) and the Notification will automatically be rendered. Here is an example: com . tweetegy . FrontDesk . notification . set ({ message : \"Everything loaded! Starting application...\" }) Conclusion I have always liked the idea of UI components and using Backbone templates, views and models really makes it easy to build a highly customized UI library for your application, department, company or even to share with the entire community! I have written another component which uses a Backbone Collection and can be used in a form to collect data from the user. This is a little more complex than the notification example here and I will cover it in a future blog post! Meanwhile, happy coding! "},{"title":"Forking Ruby Processes or How to fork Ruby","url":"/2012/04/forking-ruby-processes-or-how-to-fork-ruby/","date":"2012-04-25 00:00:00 +0700","categories":["ruby server programming"],"body":"Forking is a UNIX term that makes a process copy itself programmatically. The definition, on good old Wikipedia is: A fork in a multithreading environment means that a thread of execution is duplicated, creating a child thread from the parent thread. . Note this does not necessarily mean that a copy of the memory is allocated. In many cases the memory is shared until one of the processes actually makes a change to something then the memory: this is called copy-on-write (CoW). So with the UNIX terminology out of the way, how does this work in Ruby? How can we fork a Ruby process using Ruby? Here is a basic example: 1 puts \"This is the first line before the fork (pid #{ Process . pid } )\" 2 puts fork 3 puts \"This is the second line after the fork (pid #{ Process . pid } )\" The output from this code might be something like: This is the first line before the fork (pid 10207) 10209 This is the second line after the fork (pid 10207) This is the second line after the fork (pid 10209) So what just happened? Well the first line is pretty clear but then what is the output on the second line (10209)? Well that is the PID returned by the call to fork and , of course, represents the PID of the child process. Then we see the next two lines; one executed by the parent (with PID = 10207) and the other executed by the child (with PID = 10209). Blocks Very often, the code that developers want to run in a separate process is passed as a block to the fork method, like so: 1 puts \"You can also put forked code in a block pid: #{ Process . pid } \" 2 fork do 3 puts \"Hello from fork pid: #{ Process . pid } \" 4 end 5 puts \"The parent process just skips over it: #{ Process . pid } \" This outputs the three lines (sometimes) in different order but usally like this: You can also put forked code in a block pid: 10161 The parent process just skips over it: 10161 Hello from fork pid: 10163 Note that the PID in the block is different from the PID outside it. This is because the code running within the block is running from within the new process created by using fork. Multi-core CPU test Let’s quickly verify one of the main benefits for using fork which is for multi-processing. Let’s write a test Ruby program that can be used to demonstrate this effect. 1 def cpu_intensive_process 2 puts \"Pid: #{ Process . pid } \" 3 x = 0 4 10000000 . times do | i | 5 x = i + x 6 end 7 end 8 9 fork 10 cpu_intensive_process #should see two processors flat out! When this code is run two processors should max out. If the machine has more than two processors available then only two processors will ever max out with this code as it stands. However, using fork additional times will result in even more processes going flat out (up to all your processors, of course) ! Here is a screen capture of the CPU process monitor on my PC: Memory test So what happens with the memory allocation. Apparently in Ruby 1.9.3 (which I am using here) there is no CoW functionality which means that the memory allocation for each process should be the same. Here is a little program that goes some way towards testing that (although, I would say that it is not conclusive): 1 hash = Hash . new #load up the memory a little 2 3 1000000 . times do | i | 4 hash [ i ] = \"foo\" 5 end 6 7 puts \"Hash contains #{ hash . keys . count } keys\" 8 9 def show_memory_usage ( whoami ) 10 pid = Process . pid 11 12 mem = `pmap #{ pid } ` 13 14 puts \"Memory usage for #{ whoami } pid: #{ pid } is: #{ mem . lines . to_a . last } \" 15 16 sleep #keep the process alive 17 end 18 19 puts \"Now lets fork this process and see what memory is allocated to the child\" 20 21 puts \"Before...\" 22 23 if fork 24 show_memory_usage ( \"parent\" ) 25 else 26 27 puts \"After...\" 28 29 1000000 . times do | i | #change the values in the child memory allocation 30 hash [ i ] = \"bar\" 31 end 32 33 show_memory_usage ( \"child\" ) 34 end Here is the result of this test. Feel free to copy this code, changing it around a bit and see what results you get! Hash contains 1000000 keys Now lets fork this process and see what memory is allocated to the child Before… After… Memory usage for parent pid: 10291 is: total 62592K Memory usage for child pid: 10293 is: total 73164K Orphan Processes Finally, lets try out testing orphan processes and how they behave. Run this final little program to see: 1 fork do 2 5 . times do 3 sleep 1 4 puts \"I'm an orphan!\" 5 end 6 end 7 abort \"Parent process died...\" The results are as follows: Parent process died; terminal ~: I'm an orphan! I'm an orphan! I'm an orphan! I'm an orphan! I'm an orphan! So what's happening here? Well the parent process completes and therefore terminates to give me back the terminal prompt. But then suddenly I am interupted with the output of the 5 child processes (now technically orphans) as they execute their code one after the other following a little sleep! Well that's all for my post today on Ruby Forks or how to Fork Ruby. Next time we will be having more fun with Ruby (but not necessary forking it). Till then, happy (insert whatever it is that you do here) ! "},{"title":"Dissecting thesmallestrailsapp.com (Smallest Rails App)","url":"/2012/08/dissecting-thesmallestrailsapp-com-smallest-rails-app/","date":"2012-08-22 00:00:00 +0700","categories":["rails appplication programming"],"body":"Many of you are probably familiar with the excellent RailsCast on Rails Modularity where Ryan Bates walks through, what appears to be the ‘Smallest Rails App’ and then takes a ‘bloated’ Rails app and slims it down to it’s bare bones (but not quite as small as thesmallestrailsapp.com !). When I first watched this screencast, I have to admit that I was overwhelmed by the sheer number of Ruby techniques and tricks used to make the applications code so small. I also found it necessary to watch the first part of the screencast several times over in order to get these tricks to sink in. I have to admit that not all are terribly useful in day to day programming, however, I have decided to write a post that breaks some of these concepts down at my own speed. So here we go…. The smallest require statement First up is the first line which is the neat require statement: 1 %w(action_controller/railtie coderay markaby) . map & method ( :require ) Firstly, in case you don’t already know the syntax %w followed by brackets is a way to easily create an array of strings without having to write quotes or commas. So this will create an array like so: [\"action_controller/railtie\", \"coderay\", \"markaby\"]. As you can see, a space in the %w syntax dentoes a new element of the array of Strings. Now we have that out of the way, what does map &#038;method(:require) do? Well in real basic terms, all it does is call the method ( require in this case) passing in each element of the array to it. If we were to rewrite this line in a more verbose (and perhaps) clearer way it would look like this (more familiar) Ruby code: 1 require 'action_controller/railtie' 2 require 'coderay' 3 require 'markaby' Another Example Let me give you another example. Consider the following statement and imagine running it in an IRB session. What would the output be? 1 [ String , Array , Object ]. map & method ( :is_a? ) Well, lets break it down. This time we have an array of Ruby constants which we call map on and we pass &#038;method(:is_a?) as a block which means that each element of the Array will passed as a parameter to the is_a? method. In an IRB session, what will the receiver of the call of is_a? be then? A quick call to self reveals main and main.class #=> Object . Add so it should return [false, false, true] (since the last element in the array is Object it’s the same as calling main.is_a? Object . So this single line of code is the same as: 1 main . is_a? String #=> false 2 main . is_a? Array #=> false 3 main . is_a? Object #=> true A little contrived, I know. Another basic example which calls p \"foo\" and p \"bar\" simply outputting the string in the console. 1 %w(foo bar) . map & method ( :p ) Uses So where do you really need to use this (apart from in the smallestrailsapp) I hear you cry!? Well one good example is when you find you are writing the same code as a block over and over, you may think “I wish I could extract this into a method and call the method from within the block instead” . So, here’s a quick simple example to demonstrate this dream. Say you were finding yourself having to regularly double the elements of an array. You might find this throughout your code: 1 #Somewhere in the codesphere: 2 [ 1 , 2 , 3 ]. map { | e | e * 2 } 3 4 #Maybe in another line, method Module or Class 5 [ 3 , 5 , 6 ]. map { | e | e * 2 } What you could do (if you really think you need to) is extract the block into a method and then use the &#038;method syntax to call it on each element, like so: 1 def double ( n ) 2 n * 2 3 end 4 5 #Now pass double as a block 6 [ 1 , 2 , 3 ]. map & #038;method(:double) #=> [2,4,6] Here is a great post that explains the idea of passing methods like blocks in Ruby . Conclusion This is a great way to write succinct code in Ruby and confuse newbies! I would use it sparingly but definitely worth considering in the above case where you find that you need to reuse a block of code in multiple places. I really like the smallestrailsapp and I have taken an entire post just to discuss the first line! Can’t wait to dig into line 2 (which is empty ;)). Until then, take care and happy refactoring! "},{"title":"Mixing Ruby in Bash","url":"/2012/09/mixing-ruby-in-bash/","date":"2012-09-11 00:00:00 +0700","categories":["ruby server programming"],"body":"If your like me, I usually dread the idea of writing a bash script. Generally these days, I try to write any tasks using Ruby. Sometimes, however, it just seems that all I do is write lots of Kernel.exec or calls to sh method in Ruby! Othertimes, I just instintively start writing something in bash which is what I did today. The Original Problem I needed a startup script that would quickly get a bunch of services up and running on my development machine so that I can get coding quickly and easily. It all worked fine, but when I executed the script multiple times new instances of Ruby would be started, which is not what I wanted. I generally wanted to restart the services instead. So I used the following to kill all my ruby processes (remember, I only use this script in my development environment!): rubypids = ` pidof ruby ` sudo kill $rubypids This bash script sets $rubypids to a list of, well, Ruby pids which looks like so 2566 2899 2908 and it turns out that passing that to kill will kill them all. The New Problem The problem was that one of these pids was my instance of RubyMine that I diddnt want to kill! At that point I am thinking, damn so how to do find and replace within a string in bash? . After some unsucessful playing around with the sed command, I figured why not just use Ruby to do it instead . To my joy, Ruby works very well in bash! I used the -e option of the ruby command to do it. Here is the updated script: #Get all ruby pids and the rubymine pid allrubypids = ` pidof ruby ` rubyminepid = ` pgrep rubymine ` #Use ruby to reject the rubymine pid rubypids = ` ruby -e \"puts %w($allrubypids).reject {|pid| pid == $rubyminepid.to_s}\" ` #Kill the rest rvmsudo kill $rubypids So I fell for an Array#reject solution simply because I know how to work with Arrays in Ruby very well and because it was the first option that came to mind. I love how simple and seamless it is to ‘pass’ bash variables to Ruby and have the result of the Ruby execution passed back to bash. Maybe this can be useful again in the future when stuck with how to do something ‘simple’ (using bash) – do it in Ruby instead! "},{"title":"Using Railtie and Rails Engine in Gems","url":"/2012/09/using-railtie-and-rails-engine-in-gems/","date":"2012-09-18 00:00:00 +0700","categories":["rails appplication programming"],"body":"A (private) Gem I made recently required some code modifications that required digging into Railties and Rails Engines a little. The requirements were to inject some Javascript variables into the view on each request and to include a static Javascript file in the Asset Pipeline of the consuming Rails App (I know that this Gem will be used in Rails 3.x). Problem 1: Inject Javascript vars into the view on each render using a Gem So for the first problem I naturally chose to extend the Rails middleware stack. For problems like this I tend to work with a test Rails application to try things out first before moving the solution into a Gem. The solution for this problem (if putting the ruby file directly in the Rails lib directory) is as follows: 1 require 'rack' 2 3 class JSVars 4 5 def initialize ( app , options = {}) 6 @app = app 7 end 8 9 def call ( env ) 10 status , headers , response = @app . call ( env ) 11 12 if status != 301 & #038;&#038; response.respond_to?(:request) 13 response_string = inject_vars ( response , headers ) 14 15 response = Rack :: Response . new ( response_string , status , headers ) 16 else 17 Rack :: Response . new ( response , status , headers ) 18 end 19 end 20 21 ## inject_vars implementation not shown 22 ## all it does is add a &lt;script&gt; tag containing inline Javascript within the &lt;head&gt; tag of the html response string 23 end In addition to this code living in lib/js_vars.rb it was necessary to add a line to configure the middleware of the application to use this, as follows: 1 config . middleware . use \"JSVars\" This solution worked well directly inside the Rails app but I needed to share this functionality with numerous Rails apps so I needed it in a Gem. How can I configure middleware in a Rails app from my Gem? The answer is with a Railtie! Solution 1: Use Railties! Using Railties the solution is fortunately very simple. After moving the js_vars.rb file into the gems lib directory and removing the middleware configuration from the app, I added the following Railtie class to my Gem as follows: 1 class Railtie & lt ; Rails :: Railtie 2 initializer \"my_gem.insert_middleware\" do | app | 3 app . config . middleware . use \"MyGem::JSVars\" 4 end 5 end After adding the necessary require statements in my Gem, that was it! When I restarted my Rails app, the same functionality was working - the variables were being injected into the page as expected! Great! Now onto the next problem! Problem 2: Dynamically add assets to the Rails Asset Pipeline from a Gem I had a Javascript file that made use of these variables and I needed that to also reside within the Gem, but how to get that Javascript into the host Rails Asset Pipeline? The answer is use Rails Engines! 1 class Engine & lt ; Rails :: Engine 2 end After adding the necessary require statements for this Engine in my Gem, you guessed it, everything worked (again)! By simply including this class in the Gem it means that the Rails app the Gem is loaded into will search for assets inside the Gem! So if we add a .js file to app -> assets -> javascripts path (in the Gem) then this file can be referenced (and found!) in the Rails app as follows: 1 // = require my_gems_javascript Conclusion Rails 3.x brings us a whole set of wonderful and powerful tools: Engines, Railties, Middleware and the Asset Pipeline (as well as many other goodies!). If we want to split off and reuse functionality we can use Gems together with a mixture of these new tools. In this example, I used: Railties to extend Middleware Engines to extend the Asset Pipeline Of course this is only a small subset of the power of these tools and I highly recommend reading more about this subject. Recommended Further Reading / Watching Writing Rails Engines Getting Started Create your own Rails 3 Engine Writing a Rails Engine (YouTube video) Engines (Vimeo video by Ryan Bigg) Railties "},{"title":"Setting up a captcha with reCAPTCHA service and the captcha gem","url":"/2012/10/setting-up-a-captcha-with-recaptcha-service-and-the-captcha-gem/","date":"2012-10-22 00:00:00 +0700","categories":["rails appplication programming"],"body":"We all know about ‘captcha’. It is a very common way to prevent spam from getting into your inbox, applications, databases, servers etc. It does this by displaying some slightly distorted text such that a machine is unlikely to interpret it correctly but a human will – and this is the key. A captcha ensures that it is a real human filling out the form that will keep you busy! reCAPTCHA One of the best services to use for a captcha is Googles reCAPTCHA service . As usual, it’s possible to use this service directly via it’s API or use one of the many plugins or language wrappers available. Since I am working on a Rails app, I will select one of the Ruby implementations. I opted for the Recaptcha Gem since the Rack Middleware based gem did not work on the Rails 2.x project I am working on at the moment. Setup Add the following to your gem file: 1 gem \"recaptcha\" , :require => \"recaptcha/rails\" Then register for a reCAPTCHA API key and add that to your environment config files: 1 #put this in development.rb and in production.rb (separate keys in each so you can test!) 2 RECAPTCHA_PUBLIC_KEY = 'your-public-key' 3 RECAPTCHA_PRIVATE_KEY = 'your-private-key' The final step for setup is to add the following config file to your initializers directory: 1 #in config/initializers/recaptcha.rb 2 Recaptcha . configure do | config | 3 config . public_key = RECAPTCHA_PUBLIC_KEY 4 config . private_key = RECAPTCHA_PRIVATE_KEY 5 end View The Captcha Gem provides helpers for your view to render the actual captcha box. It’s as simple as putting the following into your view at the point where you wan the captcha to appear: 1 & lt ; %= raw recaptcha_tags %& gt ; That’s it for the view! Of course, this assumes you have put this tag into a form and that you already have error output etc. Moving on now to the server code (in the controller, of course!) Controller Code This is where the verification magic happens. The Captcha Gem provides another helper method that posts to the reCaptcha API server to check if what was submitted is correct. If it is then the method returns true, if not, it will add a custom error message that the captcha is wrong to the model instance. Here is the basic code as you might have it in the create action of your controller: 1 #....model setup from params as usual... 2 3 @model . valid? #ensures we see all errors on the model in the view if the captcha fails 4 if verify_recaptcha ( :model => @model , :message => \"Please enter the correct captcha!\" ) & #038;&#038; @model.save 5 #all ok so do as you would normally 6 else 7 #model is not valid (that includes the captcha now! probably render the form again? 8 end 9 10 #...rest of create action There are a few things happening here so let me explain. The first line to @model.valid? ensures that the models errors array is populated so that if the captcha is incorrect the user will see that message as well as all the model error validation messages. Without that only the captcha message is displayed if not entered correctly. It’s nice to show the user all the errors in one go so to prevent server round trips. The if statement then calls the verify_recaptcha helper which goes off to the Google reCAPTCHA server and validates the captcha has been enetered correctly or not. If not the message is added to the models errors array. The rest of the controller action is as one would normally write it! So essentially one has replaced: if @model.save .... with… if verify_recaptcha(:model => @model, :message => \"Please enter the correct captcha!\") &#038;&#038; @model.save pretty neat and straightforward really! Conclusion Very easy to setup, configure and use thanks to the Google service and the well written Captcha Gem! "},{"title":"Dissecting thesmallestrailsapp.com (Smallest Rails App: line 3)","url":"/2012/12/dissecting-thesmallestrailsapp-com-smallest-rails-app-line-3/","date":"2012-12-31 00:00:00 +0700","categories":["rails appplication programming"],"body":"In this post, I will carry on from where I left off in the previous post and continue my dissection of the smallest Rails app! This takes us onto line 3 of the code (since line 2 is empty!). Line 3 looks like this: 1 run TheSmallestRailsApp ||= Class . new ( Rails :: Application ) { This shows a call to Class.new which in Ruby means creating an anonymous class which inherits from the class passed into the new call. Background for Class.new The Class.new method signature is Class.new(super_class=Object) which means that you can specify the superclass by passing in the Class constant or if nothing is specified the default superclass will be Object. One thing to note with this technique is that the class is unanamed or anonymous until such point that it is assigned to a constant. In the case of The Smallest Rails App the new class is assigned to the constant TheSmallestRailsApp . One use for this would be to create a new class that inherits from another but does not have any body. A common example is when it’s necessary in a Ruby application to have a domain specific named exception available for rescue. For example: 1 SomeCustomIssue = Class . new ( StandardError ) This way its possible to rescue SomeCustomIssue within the application. Using Class.new to define this custom Error class is far cleaner than using the more common class declaration syntax which would look like this: 1 class SomeCustomIssue & lt ; StandardError 2 end Class.new can also take a block of code which will can be used to define methods (using class_eval ) in the new class. For more information on where developers might use Class.new check out fun with class.new and when to class.new The run command Working backwards we see the \"run\" command at the start of the line. This is part of Rack and only works if executed on the command line using the rackup command. Usually the Ruby code for a Rack application is saved in a file called config.ru (especially in a Rails app) however this is not necessary and can be saved as a ruby file (with .rb extension) too. Another intesting thing to note is that the run command comes from the Rack Handler Module . This means that the actual call to run could also be written as this: 1 Rack :: Handler :: Thin . run I happen to have the Thin gem installed so this runs fine on my machine. If you don't have Thin installed then you will probably see the error \"`require': cannot load such file -- thin\". This is Rack evaluating the code and trying to require thin automatically. Just install the thin gem or use a different Rack::Handler. Of course, just simply call run and Rack will use whatever handler it finds on your machine automatically. For details on how Rack does this check out the Rack source code . One error you might come across when trying to run this Rack application in your environment is \"undefined method 'append' for ActionDispatch::Routing::RouteSet\". In my case it was becasue the gemset that I was using was using an earlier version of Rails (3.0.x). Switching to another gemset with Rails 3.2.x fixed this issue. Refactorings In the Railscast, Ryan refactors this particular line to the following: 1 class TheSmallestRailsApp & lt ; Rails :: Application 2 #code in the class are on other lines so ommited here 3 end 4 5 run TheSmallestRailsApp Note above that I have not included the body code of the class. The above code is essentially the same as line 3 of the Smallest Rails App except expanded to 3 lines. It demonstrates that a call to Class.new creates a class that inherits from the constant passed to it (in this case Rails::Application) and that it is assigned to the constant TheSmallestRailsApp which is then run as a Rack app. Conclusion In line 3 we learned about Class.new, anonymous classes a little about rack applications. Next post will be about the next line of code! "},{"title":"Dissecting thesmallestrailsapp.com (Smallest Rails App: line 4)","url":"/2013/01/dissecting-thesmallestrailsapp-com-smallest-rails-app-line-4/","date":"2013-01-07 00:00:00 +0700","categories":["rails appplication programming"],"body":"In this post, I will carry on from where I left off in the previous post and continue my dissection of the Smallest Rails app! This takes us onto line 4 of the code. Line 4 looks like this: 1 config . secret_token = routes . append { This shows the Rails config secret_token being set to return value of routes.append. What’s going on here then and why are we even doing this? Rails config.secret_token In order for Rails to process requests config.secret_token must be set to a random string of at least 30 chars that is difficult (near impossible) to guess. The purpose of the secret key is to verify the integrity of signed cookies – without it Rails will fail to process requests with the following 500 error: <br /> ArgumentError (A secret is required to generate an integrity hash for cookie session data. Use config.secret_token = \"some secret phrase of at least 30 characters\" in config/initializers/secret_token.rb)<br /> This is a generic error message since the secret_token in the Smallest Rails App is set in config.ru file. Notice the comment “some secret phrase of at least 30 characters” which means the phrase can be any string (30 chars or more in length). In the Smallest Rails App this is set to the string value of the routes.append block (notice the call to to_s in line 14). On my machine this returns the following string : <br /> [#<Proc:0x007fa5842838c8@/thesmallestrailsapp.com/config.ru:5>]<br /> Note that in a conventional Rails App the secret_token will be generated automatically and will look something like this 0eb7e863b78c7bdc52376bc0f24b2c634fa8e1ca83e2d7b695da86b44e80194..... . If you need to regenerate the secret_key you can run SecureRandom.hex(64) in the console and copy the output to your initializer file or run rake secret which does this same thing without having to log into the Rails console. 1 rake secret 2 Generate a cryptographically secure secret key ( this is typically used to generate a secret for cookie sessions ) . Warning : if you change this key you will need to restart your application. Additionally, existing signed cookies will become invalid effectively throwing currently logged in or in session users off your system! It’s probably not necessary to ever change this key unless you think your applications security has been compromised. Secure your Secret Token A quick note about securing your secret token. If your project is open source or you have a number of developers working on a private repo then you should not commit the actual production secret token into your source control repository! Since the secret token can be used to generate signed cookies then it is possible for a hacker to create valid cookies for an admin or other user of your application. The recent SQL injection vulnerability in Rails used knowledge of the secret token to achieve this. The following post on keeping your secret token a secret has more on this subject! Rails routes.append Rails Routes is a fundamental part of the Rails framework. It tells Rails what to execute based on the pattern of the incoming request url. Naturally, even the Smallest Rails App needs at least one route! In this case it is the root route that is being configured (on the next line actually, line 5). For now lets briefly investigate what exactly routes.append does. routes.append works in a very similar way to routes.draw . The only apparent difference is that routes.draw does not work before the application is initialized. As far as I can tell routes.append (and the routes.prepend cousin) allow for new Rails routes to be added at runtime . In conventional Rails applications routes are ‘drawn’ in the routes.rb file by passing a block to the draw method call. Within the block, developers have access to a DSL for defining their applications routing. Some of the commonly used methods in the DSL are root (as used in the Smallest Rails App) for defining the root url (‘/’) action within the application. Another very common method call is to resources which is a very powerful way to create a RESTful endpoint for a particular resource within your application. Rails routing is a big topic and for further information it is best to read the Rails Guide to Routing . Conclusion Line 4 of the Smallest Rails App introduces us to a few new concepts namely the secret_token and Rails routes. In the next post, I will cover line 5 of the application. "},{"title":"Create Nested Polymorphic Comments with Rails and Ancestry Gem","url":"/2013/04/create-nested-comments-in-rails-using-ancestry-gem/","date":"2013-04-04 00:00:00 +0700","categories":["rails appplication programming"],"body":"I know the post title sounds a little complex and a real mouth full but, hey, its just a commenting solution that can be applied to any resource in a Rails application. Additionally, comments can be commented on (hence ‘nested polymorphic comments’). By the way, this post comes with a full working Rails Application example available on Github. The code is not exactly the same as shown below but the application should work if you follow the README! So, therefore, in this post I am going to build an application that uses the Ancestry Gem to enable a hand rolled nested commenting system! You can think of this as an example that combines the techniques demonstrated in RailsCasts episode 154 – polymorphic associations and RailsCasts episode 262 – trees with ancestry. Installation First up, install the excellent Ancestry Gem . I am using Rails 3 so this is what I will demonstrate here. As always, its pretty straightforward to install and setup. Simply, Add this to your Gemfile (and, of course, run “bundle” to install it!) 1 gem 'ancestry' Add the ancestry column to any tables that require it. In my application, I’ll add it to comments as part of the “change” migration since this is a brand new application. Here is how my migration looks after manually adding the index (obviously run rake db:migrate to update your database with these changes). Note that I add commentable_id and commentable_type so that comments is polymorphic and can be added as a has_many association to any model that requires comments. 1 class CreateComments & lt ; ActiveRecord :: Migration 2 def change 3 create_table :comments do | t | 4 t . text :content 5 t . integer :commentable_id 6 t . string :commentable_type 7 t . string :ancestry 8 9 t . timestamps 10 end 11 12 add_index :comments , :ancestry 13 end 14 end Add the has_ancestry call to your model, like so: 1 class Comment & lt ; ActiveRecord :: Base 2 has_ancestry 3 end Additionally, add has_comments to any model that you want to make 'commentable'. Let's make this application something related to movies (as I am real fan!). In this case I'll make the Movie model commentable by adding a has_many association to it like so: 1 class Movie & lt ; ActiveRecord :: Base 2 has_many :comments , :as => :commentable , :dependent => :destroy 3 end Thats it for the setup! Rendering out some nested comments Since the main benefit of using the Ancestry gem is how it stores data in a tree like structure, we'll get straight on to displaying our comments as a nested tree. Firstly create this helper method. This solution was inspired from the following RailsCast on this subject: 1 module CommentsHelper 2 def nested_comments ( comments ) 3 comments . map do | comment , sub_comments | 4 content_tag ( :div , render ( comment ), :class => \"media\" ) 5 end . join . html_safe 6 end 7 end Next, call this helper method from your show view to render the nested comments! 1 = nested_comments @comments The actual comment partial (which will live in views/comments/_comment.html.haml ) looks like this (note the recursive call to nested_comments in the last line!). 1 . media - body 2 % h4 . media - heading = comment . user . name 3 % i = comment . created_at . strftime ( '%b %d, %Y at %H:%M' ) 4 % p = simple_format comment . content 5 = nested_comments comment . children Adding a new comment / thread Now lets look at how to create a new thread. What we need is to render at the bottom of our show view, right after the nested_comments call, a form to start a new comment thread. We'll render a shared partial in this case since we will want to use the same form for any thing that is commentable: 1 #In the show view 2 = render \"comments/form\" 3 4 # In comments/_form partial 5 = form_for [ @commentable , @comment ] do | f | 6 = f . hidden_field :parent_id 7 % p 8 = f . label :content , \"New comment\" 9 % br 10 = f . text_area :content , :rows => 4 11 % p 12 = f . submit \"Post Comment\" You might now be wondering where are the instance variables @commentable and @comment being set? Well we could set these directly in the show action of the MoviesController , however, that would quickly lead to code duplication when we want to make another model commentable. How about we make a module and include that into our controller instead? That way any model that is commentable, we only need to include the module into the corresponding controller. Here is the code for the Commentable module: 1 require 'active_support/concern' 2 3 module FilmFan::Commentable 4 extend ActiveSupport :: Concern 5 6 included do 7 before_filter :comments , only => [ :show ] 8 end 9 10 def comments 11 @commentable = find_commentable 12 @comments = @commentable . comments . arrange ( :order => :created_at ) 13 @comment = Comment . new 14 end 15 16 private 17 18 def find_commentable 19 return params [ :controller ]. singularize . classify . constantize . find ( params [ :id ] ) 20 end 21 22 end 23 24 # In MoviesController 25 class MoviesController & lt ; ApplicationController 26 include FilmFan :: Commentable 27 end Thats nice as it cleans up our Controllers show actions. However, there is still an issue! The form partial form_for [@commentable, @comment] declaration means that we need to post to a generic CommentsController each time - not the actual controller that is \"commetnable\". This means we need to create this controller and provide a create method in it. 1 class CommentsController & lt ; ApplicationController 2 def create 3 @commentable = find_commentable 4 @comment = @commentable . comments . build ( params [ :comment ] ) 5 if @comment . save 6 flash [ :notice ] = \"Successfully created comment.\" 7 redirect_to @commentable 8 else 9 flash [ :error ] = \"Error adding comment.\" 10 end 11 end 12 13 private 14 def find_commentable 15 params . each do | name , value | 16 if name =~ /(.+)_id$/ 17 return $1 . classify . constantize . find ( value ) 18 end 19 end 20 nil 21 end 22 end Note the find_commentable method courtasy of RailsCast episode 262 . Adding nested comments Now lets combine the 'polymorphic' with the 'ancestor' and thus add the functionality to create a comment of a comment! The first thing to do is add a 'reply' link in our comment partial. This can go just above the recursive call to nested_comments so that we get this link rendered just after each comment. Note the use of the helper method new_polymorphic_path and that we still use the @commentable instance but in this case we pass an instance of Comment.new so that link will call the CommentController#new action. Note also that we are passing in a parent_id which the Ancestry gem requires for building the tree. 1 # start of partial ommitted.... 2 . actions 3 = link_to \"Reply\" , new_polymorphic_path ( [ @commentable , Comment . new ] , :parent_id => comment ) 4 = nested_comments comment . children As mentioned above, when the 'Reply' link is clicked the new action in the CommentsController is called. The method takes the commentable type, finds the actual instance of that using the private method find_commentable (shown above) and creates and new instance of Comment based on these parameters. The method looks like so: 1 class CommentsController & lt ; ApplicationController 2 def new 3 @parent_id = params . delete ( :parent_id ) 4 @commentable = find_commentable 5 @comment = Comment . new ( :parent_id => @parent_id , 6 :commentable_id => @commentable . id , 7 :commentable_type => @commentable . class . to_s ) 8 end The final piece of the puzzle is the form which needs to be rendered here. While it would be more desirable to render the form inline using an Ajax solution, here we will just simply render another view with the corresponding form that contains the new Comment object (with the correct commentable_id , commentable_type and parent_id all set, of course!). 1 # In comments/new.html.haml 2 % h1 New Comment 3 = render 'form' Actually, the form that is rendered is the same comments form partial as shown above. Of course, in this case the @comment instance variable now represents a new child comment since its parent id is now set! Enabling another resource 'commentable' Now lets add an Actor to the mix with the association Movie <= has_and_belongs_to_many => Actors . How would we make the Actor resource commentable? Simply follow these steps: Add \"has_many :comments, :as => :commentable, :dependent => :destroy\" to the Actor model Include the FilmFan::Commentable into the ActorsController Add the \"nested_comments @comments\" and \"render \"comments/form\" calls to the base of the Actor show view Finally, add the nested comments resource route for actors in your routes file, just as you did with movies. Conclusion I hope you enjoyed this post and that you find this useful in your own projects. Here is a link to the Github project with a full working Rails Application example . "},{"title":"Testing infinite scroll using RSpec and Capybara (without sleep or wait_until)","url":"/2013/05/testing-infinite-scroll-using-rspec-without-sleep-or-wait-until/","date":"2013-05-31 00:00:00 +0700","categories":["rails appplication programming"],"body":"Recently I added some functionality to an index page that required infinite scrolling. The idea is that on the index view as the user scrolls down the page, and reaches near to the bottom of that page, an Ajax request is made to the server to fetch the next set of records. In my opinion for this type of view it is much nicer than using pagination (although the solution can still use a pagination Gem in the background, in my case Kaminari ). RSpec / Capybara I will not go into the implementation which is really quite trivial. There is an excellent RailsCast available for ‘Endless Page’ solution available already. However, I will demonstrate here how I tested this using RSpec & Capybara. Below is the spec: 1 describe \"People index\" do 2 let! ( :person ) { create ( :person ) } 3 4 before do 5 visit people_path 6 end 7 8 it \"loads more records when scroll to bottom of page\" , :js => true do 9 default_per_page = Kaminari . config . default_per_page 10 create_list ( :person , default_per_page ) 11 Person . count . should > default_per_page 12 visit current_path 13 page . should have_css ( '.table-index tbody tr' , :count => default_per_page ) 14 page . execute_script ( 'window.scrollTo(0,100000)' ) 15 page . should have_css ( '.table-index tbody tr' , :count => Person . count ) 16 end 17 end Firstly notice that we set default_per_page to our pagination engine (in this case Kaminari). Then we create that many resources (here its Person resource). The idea is that when Capybara visits the index page ( people_path ) there will be (at least) one resource not loaded on that page yet (until scrolling to the bottom of the page). The important thing is to create at least default_per_page + 1 resources so that this is testable. We confirm that there are indeed more resources in the database than the default_per_page with the following line: 1 Person . count . should > default_per_page Once we load the people index page, an assertion is made that the page should contain the same number of resources as defined in the default_per_page variable. This is what we expect from our pagination solution anyway. 1 page . should have_css ( '.table-index tbody tr' , :count => default_per_page ) The following line actually scrolls the page down to the bottom which should trigger the Ajax request to load more records. 1 page . execute_script ( 'window.scrollTo(0,100000)' ) This is then asserted in the next line that the page now contains People.count rows in the table. 1 page . should have_css ( '.table-index tbody tr' , :count => Person . count ) Sleep(ing) on the job My first version of this spec was always failing until I put a little sleep call before the final assertion. I initially thought this might be necessary to let the DOM properly load following the Ajax call. It turns out, however, that the sleep call was required because I was not writing the assert in the correct way. Here is how the last three lines of my spec looked ( don’t do this! ): 1 #rest of spec is the same as above example 2 page . all ( '.table-index tbody tr' ) . count . should eq default_per_page 3 page . execute_script ( 'window.scrollTo(0,100000)' ) #scroll to the bottom of the page 4 sleep 1 #NOTE: THIS SHOULD NOT BE NECESSARY. 5 page . all ( '.table-index tbody tr' ) . count . should eq Person . count #NOTE: THIS IS NOT THE CORRECT WAY TO ASSERT THIS! The problem is in calling page.all passing in a selector like this runs immediately and the page is indeed not given a chance to update following the Ajax request. This is why I added a sleep call before it. But wait…I checked on the Capybara Cheat Sheet and found that there was a wait_unitl method so maybe that is what I need! Capybara wait_until So I drop the wait_until method into my spec as follows: 1 #rest of spec is the same as above example 2 page . all ( '.table-index tbody tr' ) . count . should eq default_per_page 3 page . execute_script ( 'window.scrollTo(0,100000)' ) #scroll to the bottom of the page 4 wait_until { page . has_content? ( Person . last . name ) } #NOTE: THIS DOES NOT WORK IN CAPYBARA 2.x 5 page . all ( '.table-index tbody tr' ) . count . should eq Person . count However, to my surprise I get the following runtime error when I execute the spec: 1 NoMethodError : undefined method 'wait_until' for #&lt;RSpec::Core::ExampleGroup::N... After some quick Googling I discovered that wait_until has been removed from Capybara 2.0.0 (I am using Capybara 2.1.0, in fact). It was indeed this blog post that led me to realize I was writing my assertion incorrectly and caused me to refactor it to its final solution: 1 page . should have_css ( '.table-index tbody tr' , :count => Person . count ) This implicitly waits for the content to load (or timesout). For more details on exactly how this works, how to change the timeout etc, I recommend reading the suggested blog post on this subject (or the Capybara Docs ). Conclusion I have to admit that when I wrote the spec initially using the sleep method it did smell bad. This is why I went ahead and did a little research into better ways of doing this. If the wait_until method was still in Capybara I would have probably (still in error) used that. However, it is a joy to learn that CSS selector methods such as have_css implicitly wait until the DOM is loaded making the spec much cleaner! "},{"title":"Dissecting thesmallestrailsapp.com (Smallest Rails App: Newsflash)","url":"/2014/03/dissecting-thesmallestrailsapp-com-smallest-rails-app-newsflash/","date":"2014-03-04 00:00:00 +0700","categories":["rails appplication programming"],"body":"Since starting this series, the inevitable has happened – The Smallest Rails app has been refactored! In this post I want to cover the recfactorings to the first line which affects the first blog post in this series . To recap the first line was as follows: 1 %w(action_controller/railtie coderay markaby) . map & method ( :require ) Now this has been changed to the following (2) lines (Note: since writing this post the first line has been removed (see conclusion below): 1 require \"bundler/setup\" 2 Bundler . require For clarity, here is the commit diff in Github for this change. Compare with Initial Commit My first thought was that maybe this application was not using Bundler (before), but I can confirm that the first commit of this project was and even had the line require 'bundler/setup' so its been used all the time. For your information the initial commit of this project has a commit SHA of “2215c1a” and if you want to see how this looked just simply clone the project and checkout that commit to detach your code from HEAD, as follows: git clone git@github.com:artemave/thesmallestrailsapp.com.git git checkout 2215c1a63700f21d54d0d16466f5527932ac8df6 As you can see the application looks quite different from what it does now, but it does appear to be using Bundler. Perhaps another interesting series of posts would be to track the progress of changes to this app discussing the reason for each one, but maybe another day, for now let simply focus on the two lines that changed here. By the way, if you want to get back to the HEAD for the repo, just run git checkout master Bundler Setup The first line require 'bundler/setup' simply runs: this code which, in turn, runs the Bundler.setup method. What does this do? According to the Bundler Docs it …configures the load path so all dependencies in your Gemfile can be required… . Essentially it means that once Bundler.setup has run, its then possible to require any gems as declared in the Gemfile using require and be certain that the correct bundled version of that gem will be loaded. So Bundler.setup simply ensures that the load path is correctly set so that require xyz gem will load the very gem as defined in the Gemfile.lock file. Bundler.require The next line does exactly what it says, it requires all the gems! In fact, this is a little known command that will require all the gems in the Gemfile. The alternative is to require each gem one by one, in this case ‘rails, ‘coderay’ and ‘markaby’. However, since this is the Smallest Rails app, we use Bundler.require to require all gems in one swoop! Its also possible to execute the require method passing in a symbol representing the group that you desire to load. For example Bundle.require(:development) will require all declared gems in the development group of your Gemfile. Its possible to pass in multiple groups separated by comma too. Bundler.setup and ‘bundle exec’ Its worth mentioning here that running ‘bundle exec’ on the command line before starting the Smallest Rails App ensures that the load path is setup correctly (essentially the same as require \"bundler/setup\" ) and thus means that its possible to run the Smallest Rails App without the require \"bundler/setup\" line. I have commented on this line of code to the author of the Smallest Rails App and he has since removed this line from the application! Conclusion The Smallest Rails app will still probably be able to get smaller as gems and approaches change. In the meantime, for our purpose, it serves as an interesting digest of new techniques to learn about Rails and its associated gems (in this case Bundler!). "},{"title":"Legacy Rails app (well 2.3.x) with (almost) latest Rspec, Capybara &#038; Factory Girl","url":"/2014/03/legacy-rails-app-well-2-3-x-almost-latest-rspec-capybara-factory-girl/","date":"2014-03-11 00:00:00 +0700","categories":["rails appplication programming"],"body":"I recently had a look at an oldish Rails project that uses Ruby version 1.8.7 and is running version 2.3.11 of Rails. One of the problems with this project was that it is completly untested! So I thought, since I prefer TDD (as it actually keeps me sane!) I would setup Rspec, Capybara and Factory Girl for this project. My initial thoughts were that this would not be easy since it was an old version of Rails tied (currently) to Ruby 1.8.7. Fortunately, this turned out not to be the case and I found that Rspec version 2.11.0 happily works! Why I like TDD? Just a quick note on why I like TDD and why I say it keeps me ‘sane’. The main reasons are: I know my stuff works I can refactor with confidence Its easier to develop and debug in test environment I can sleep well at night! Anyway, enough of that, lets get onto the main topic of question here… Gemfile Now I can add the remaining gems directly to the Gemfile like so: 1 group :test do 2 gem 'rspec' , '2.11.0' 3 gem 'capybara' , '1.1.4' 4 gem 'pry' 5 gem 'factory_girl' 6 gem 'database_cleaner' 7 gem 'shoulda-matchers' 8 end So then all that was left was to let Bundler do the heavy lifting for finding and associating the correct versions of the Gem dependencies for all of these! Sample Rails 2.3.11 project I have made a simple Rails 2.3.11 project that uses the above Gems and has some example specs to show that it does indeed work. It can be downloaded here: rspec_legacy_rails You will notice that we have had to explicitly set certain gem versions like nokogiri and mime-types etc. This is mainly due to the project using ruby 1.8.7 and most of these gems complain about that and ask for ruby 1.9.2 (or higher). While it would be better to upgrade ruby, since we assume, this project does not have any tests (for now) we’ll keep the version of Ruby set as it is. However, generally, for projects like this, I would spend a week or two getting a decent amount of the features (not all) covered with specs and then upgrade Ruby then upgrade Rails. The thing that is great about this approach so far is that we are able to use a relatively new version of our testing frameworks. This means we don’t have to rewrite specs again once we upgrade. We should be able to fix any issues associated with the upgrade, maybe do some refactoring and then we are good to go! Spec Helper File This is probably the most important file for setup of the environment. What I have put into the file can probably be refactored. There are some lines I need to explain briefly too: 1 # This ensures that all the gems are required before we begin. Saves writing 'require ...' for each gem! 2 Bundler . require ( :test ) 1 # This ensures that we have access to all the url helpers in the specs. 2 config . include ActionController :: UrlWriter 1 # This ensures that we can use the Capybara DSL methods like 'visit' for our integration specs 2 config . include Capybara :: DSL Binding.pry and save_and_open_page Another cool thing is that both binding.pry and save_and_open_page both work in this environment. Conclusion The main point of all this is that while the project was ‘legacy’ Rails, using an ‘ancient’ version of Ruby and (worst of all) completely untested, I was able to set myself up with a comfortable test environment which (almost) resembled my environment for some more modern Rails (think version 4.x) applications that I am involved in. This allowed me to get on and add features, refactor and deploy with confidence (and sleep at night too!). One other thing to note is that I have ONLY tried out Model and Integration (or Feature) specs in this app. This is mainly becuase these are the main set of spec types that I usually work with. Having said that, the Controller specs would be useful if we need to work with legacy Rails application that expose a RESTful API for a single page app, for example. Unfortunately, I can confirm that Controller specs do not appear to work in this setup, but feel free to have a go! "},{"title":"Testing Rails ActionMailer using RSpec Shared Examples","url":"/2014/03/testing-mailers-using-rspec-shared-examples/","date":"2014-03-21 00:00:00 +0700","categories":["rails appplication programming"],"body":"I love ActionMailer in Rails, for many reasons, but one of them is that they are easy to test! I work with mailers a lot in many projects and I do find that I often follow similar testing patterns for each mailer in each project. In this post, I’ll demonstrate using RSpec shared examples to reuse some of the mailer specs across multiple mailers. An ActionMailer What does it take to create an ActionMailer in Rails 4? A generator, of course! 1 rails g mailer CoolMailer email What you should end up with is a basic Mailer class that inherits from ActionMailer::Base and related views (+ specs, of course!). Testing these puppies! What we need to do now is actually test these puppies. As we know there are a large number of similarities between most mailers. For example, the from property needs to be set to something appropriate for your application or context. The to property also needs to be set, of course! We usually want the email to contain a body and we usually want to check that certain content is rendered (or not) within it. Sometimes we want to send multipart emails for supporting both text and html clients. Other things that may be generic mailer tests: Testing that certain headers are set like ‘X-Priority’ and ‘X-MSMail-Priority’ Attachments Header content type Subject text Body text The way I test most of this is using a shared example similar to the one below. 1 shared_examples \"a well tested mailer\" do 2 let ( :greeting ) { \"Darren\" } 3 let ( :full_subject ) { \" #{ asserted_subject } :You got mail!\" } 4 let ( :mail ) { mailer_class . email ( greeting ) } 5 6 it \"renders the headers\" do 7 mail . content_type . should start_with ( 'multipart/alternative' ) #html / text support 8 end 9 10 it \"sets the correct subject\" do 11 mail . subject . should eq ( full_subject ) 12 end 13 14 it \"includes asserted_body in the body of the email\" do 15 asserted_body . each do | content | 16 mail . body . encoded . should match ( content ) 17 end 18 end 19 20 it \"should be from 'from@example.com'\" do 21 mail . from . should include ( 'from@example.com' ) 22 end 23 end Somethings I want to point out about this shared example: Its possible to test multiple content assertions in one go by passing in an array of different content expected. I usually do this for the body. Same goes with the subject. We can test that the subject contains some certain text in the context of our mailer We can swap out different mailers by setting the mailer_class variable to a different class (as long, in this case, the template is called ‘email’ although this can also be dynamic too if we like). Notice that we can treat this shared example, as all shared examples can, like a method call where we ‘pass in’ certain parameters to work against. You may notice in the above shared examples, variables listed that are not explicitly declared like full_subject , asserted_body and mailer_class . These are set outside of the shared example in the calling spec as shown below: 1 describe CoolMailer do 2 describe \"email\" do 3 let ( :mailer_class ) { CoolMailer } 4 let ( :asserted_subject ) { \"A cool mail!\" } 5 let ( :asserted_body ) { [ \"This content\" , \"That content\" ] } 6 7 it_behaves_like \"a well tested mailer\" do 8 end 9 end 10 end A word on multipart html/text emails You will notice in the shared example a spec for the headers to start with ‘multipart/alternative’. This ensures that our emails are being sent out in both plain text and html. How do we actually get this spec to pass? Fortunately this just works out of the box with ActionMailer! All we need to do is provide 2 templates ; one for html and one for plain text as follows: #email.html.erb <h1> CoolMailer#email </h1> #email.text.erb CoolMailer#email The main thing that gets this to work is good old Rails ‘convention over configuration’. By simply placing the ‘.text.erb’ template and ‘.html.erb’ template with the same name (‘email’) ActionMailer automaticlly sends out a multipart email! Its that easy! Try removing the ‘.text.erb’ template and you will see the header spec fail with: expected \"text/html; charset=UTF-8\" to start with \"multipart/alternative\" Conclusion The purpose of this post was to demonstrate two things: testing mailers and using shared examples. You don’t have to use shared examples to test mailers, especially if you only have one mailer in your project. However, most large Rails applications will have multiple mailers so using shared examples makes sense. As always, an example application is available for download here: Testing Mailers example application "},{"title":"Rails View Template Inheritance","url":"/2014/04/rails-view-template-inheritance/","date":"2014-04-02 00:00:00 +0700","categories":["rails appplication programming"],"body":"View Inheritance has been in Rails since version 3.1. In this post, I cover a Rails 4 example and as far as I know what is covered here in this blog and in the example application is compatible with the earliest version of View Inheritance in Rails 3.x. View inheritance maps to Controller inheritance In a Rails application, one will generally inherit most of the applications controllers from ApplicationController . Lets say a very basic course management application will have Category, Course and Tutor controllers then, by default, each controller will inherit from ApplicationController. Rails will also expect a corresponding view folder for each controller like so: views -> categories #folder for CategoriesController courses #folder for CoursesController tutors #folder for TutorsController Passing in actions to the Rails Controller generator will also create corresponding view templates for those stated actions under the corresponding view directory: 1 rails g controller categories index show 2 3 # This creates the following Controller: 4 5 class CategoriesController & lt ; ApplicationController 6 def index 7 end 8 9 def show 10 end 11 end 12 13 # ... and it will also create index.html.erb 14 # and show.html.erb under views -> categories folder 15 # (as well as helpers, assets, specs etc) The standard behaviour for this application is to visit the index for a resource, like /categories which will flow through the application eventually calling the index method on CategoriesController and by Rails conventions, it will render the ‘app/views/categories/index.html.erb’ template. Delete the categories/index.html.erb Go on! Delete the index.html.erb template under views/categories folder! Don’t be scared. In fact you will discover something quite interesting in the error message that is returnd by Rails that gives us a clue to how view template inheritance works. I’ve already tried it, here is the part of the error message that spills the view template inheritance beans: Missing template categories/index, application/index with …# blah blah Notice that Rails looks in two places for our template, both in ‘categories/index’ (as we expect) and ‘application/index’. What this means is that if we place an index.html.erb file in ‘views/application’ directory (keeping the index file deleted under ‘categories’) then Rails will render the index.html.erb file from ‘views/application’ folder when we visit /categories ! Real World Example This approach can be useful for exactly what it says on the tin ‘inheritance’. If we wanted, we could place some common view output inside a template that is under the ‘views/application’ directory. A common example might be a menu for your application that is common for most of the application but maybe slightly different for other parts. Here is an example where we use partials to achieve this: views -> application -> _menu.html.erb categories -> index.html.erb courses -> index.html.erb tutors -> _menu.html.erb index.html.erb In the above example Rails application views folder structure we have each folder with an index page and each index page renders out the menu partial like so: 1 render 'menu' In categories and courses, the menu partial is not present in the categories or courses views directory so Rails will use view inheritance and look up the tree. Here is the key : because CategoriesController and CoursesController inherit from ApplicationController the next logical folder that Rails will search for the menu partial is under ‘views/application’. What we end up with is an application which renders the same menu partial on every page except where it is overridden, in this case under tutors (maybe to add a specific apply for teaching role link or something!). Here are the two menu partial templates, one under ‘views/application/’ and the other under ‘views/tutors/’ 1 # In 'views/application/_menu.html.erb' 2 < ul > 3 < li > 4 & lt ; %= link_to \"Home\", root_path %> 5 </li> 6 7 <li> 8 &lt;%= link_to \"Categories\" , categories_path %> 9 </li> 10 11 < li > 12 & lt ; %= link_to \"Courses\", courses_path %> 13 </li> 14 15 <li> 16 &lt;%= link_to \"Tutors\" , tutors_path %> 17 </li> 18 19 < /ul> 1 # In 'views/tutors/_menu.html.erb' (note extra menu item at the bottom) 2 < ul > 3 < li > 4 & lt ; %= link_to \"Home\", root_path %> 5 </li> 6 7 <li> 8 &lt;%= link_to \"Categories\" , categories_path %> 9 </li> 10 11 < li > 12 & lt ; %= link_to \"Courses\", courses_path %> 13 </li> 14 15 <li> 16 &lt;%= link_to \"Tutors\" , tutors_path %> 17 </li> 18 19 < li > 20 < b >< a href = '#' > Apply for teaching position! < /a></ b > 21 < /li> 22 23 </u l > Removing the duplication In the example above we have duplicated the first set of menu items in the ‘tutors/_menu.html.erb’ file. Ideally, it would be nice to avoid that duplication and just add the extra menu items that we need. The trick is to use a placeholder for where we want the extra items to appear (in this case at the end). We also need to provide an empty placeholder template in the ‘views/application’ directory so that Rails does not throw a missing template error. What we end up with is the following in our templates: 1 # In 'views/application/_menu.html.erb' 2 < ul > 3 < li > 4 & lt ; %= link_to \"Home\", root_path %> 5 </li> 6 7 <li> 8 &lt;%= link_to \"Categories\" , categories_path %> 9 </li> 10 11 < li > 12 & lt ; %= link_to \"Courses\", courses_path %> 13 </li> 14 15 <li> 16 &lt;%= link_to \"Tutors\" , tutors_path %> 17 </li> 18 & lt ; %= render 'appended_menu' %> 19 # './application/_appended_menu.html.erb' is an empty file 20 21 </ul> Now we can delete the menu partial under ‘views/tutors’ and instead add the ‘appended_menu’ partial as follows: 1 # In 'views/tutors/_appended_menu.html.erb' 2 < li > 3 < b >< a href = '#' > Apply for teaching position! < /a></ b > 4 < /li> So now thanks to Rails view inheritance the application will run without error and for the tutors page the application will render the tutors version of the appended_menu partial thus adding our additional link to the page all while keeping the views dry! Sweet! As you can probably tell this approach only works if we need to append something to the menu. What happens if we want to put this link before the tutors link? We have a few options: Simply move the replaceable partial placeholder before the tutors link in the application menu partial Keep the ‘append_menu’ partial where it is (for appending menu items) and make a new partial specifically before tutors link If you find you need a lot of custom template overriding like this, use the deface gem (or similar). Going further up the tree! Here is quick example that demonstrates a namespaced controller under the namespace Admin. So we create a new Admin::TutorsController which we want to inherit from Admin::BaseController using the following generator command: rails g controller admin/base rails g controller admin/tutors index We end up with a slight change to our views folder structure, namely the following has been added: views -> admin -> base -> tutors -> index.html.erb We also manually inherit the Admin::TutorsController from Admin::BaseController like so: 1 module Admin 2 class TutorsController & lt ; BaseController 3 def index 4 end 5 end 6 end Now we can still render the menu partial from the admin/tutors/index template and Rails will fallback to the partial under 'views/application' because Admin::TutorsController inherits from Admin::BaseController which, in turn, inherits from ApplicationController ! Now comes the inheritance magic! If we place an 'appended_menu' partial under 'views/admin/base' then this partial will be rendered to add additional menu items but only when the user visits any path under the admin namespace. So its possible to place administrator links here like 'manage tutors' or whatever and these will only be rendered when visiting 'admin/tutors/' path in the browser and not '/tutors'. Try out the sample app (link below) for a demonstration of this. Conclusion View inheritance can be really powerful and useful for your application. Its a nice way to dry up views and keep partials and content in their place. If you need to perform more complex view manipulations you can always use the deface gem . An example application to demonstrate this can be downloaded from here: View Template Inheritance "},{"title":"Using postgres hstore with Rails / Active Record","url":"/2014/06/using-postgres-hstore-rails-active-record/","date":"2014-06-04 00:00:00 +0700","categories":["databases","nosql","rails appplication programming"],"body":"One good solution for implementing key / value persistence to an Active Record model is to use a postgres hstore column. This gives you the ability to store any key / value pair (as strings) directly on the model and manipulate them as first class attributes. In this post, I’ll demonstrate how to set up a Rails application to use hstore and I’ll also show some of the pitfalls and limitations of this technique. Enable your app to use hstore Once you have postgres installed, the only requirement is to enable hstore in the database. This can be done by running the following command in postgres: CREATE EXTENSION hstore ; This can go in a migration too, of course, which is recommended. Just create the migration file in the usual way and add the above statement to the up method of the migration and add the DROP EXTENSION equivalent to the down method of the migration. Here is an example: 1 class SetupHstore < ActiveRecord :: Migration 2 def self . up 3 execute \"CREATE EXTENSION IF NOT EXISTS hstore\" 4 end 5 6 def self . down 7 execute \"DROP EXTENSION IF EXISTS hstore\" 8 end 9 end Now we can create a migration for our model that we want to use hstore on. A typical example is where we might have a single 'thing' (lets say Product) that have some common attributes (like name, price) but also have different properties depending on the actual product (like 'dpi' specifically for a camera but not for shoe for example!). So lets look at an example migration for Product that contains a properties hstore column: 1 class CreateProducts < ActiveRecord :: Migration 2 def change 3 create_table :products do | t | 4 t . string :name 5 t . string :description 6 t . float :price 7 t . hstore :properties 8 9 t . timestamps 10 end 11 end 12 end As you can see from t.hstore :properties ; hstore is treated in the migration a first class database type. Run the migrations and this will enable the hstore extension in your database and add the products table with the hstore column. Playing around with hstore in the console Lets start with getting a feel for hstore in the console in order to understand what it can do for us and what its limitations are. The following code will create a new Product item leaving properties nil. 1 Product . create ( name : \"Cannon D30\" , description : \"A great digital SLR\" , price : 399 . 99 ) 2 Product . first . properties # => nil Lets set some key / value items on properties. Lets add the following ( ): Sensor Format: APS-C CMOS Megapixels: 3.1 Min ISO: 100 Max ISO: 1600 Here is the code to update the properties on this product instance: 1 camera_properties = { sensor_format : \"APS-C CMOS\" , megapixels : 3 . 1 , min_iso : 100 , max_iso : 1600 } 2 p = Product . first 3 p . update ( properties : camera_properties ) Now, if we inspect the properties attribute on the product the output is: 1 p . properties #=> {:sensor_format=>\"APS-C CMOS\", :megapixels=>3.1, :min_iso=>100, :max_iso=>1600} Interestingly, if we call reload we see a slightly different output, namely that both the keys and values are now all strings: 1 p . reload . properties #=> {\"max_iso\"=>\"1600\", \"min_iso\"=>\"100\", \"megapixels\"=>\"3.1\", \"sensor_format\"=>\"APS-C CMOS\"} This is because the postgres database stores all hstore key / values as strings regardless of the type in Ruby. Inspecting the database row shows this: select name , properties from products ; name | properties -------------+----------------------------------------------------------------------------------------- Cannon D30 | \"max_iso\" => \"1600\" , \"min_iso\" => \"100\" , \"megapixels\" => \"3.1\" , \"sensor_format\" => \"APS-C CMOS\" ( 1 row ) Adding more properties to an existing model The cool thing about using hstore is that we can add new attributes to the column very easily. For example we might decide that we want to include information about the storage format of the camera. We can easily do this by adding this key / value to the properties. The important thing to note is that we must merge the new properties into the existing hash: 1 p . update ( properties : p . properties . merge ({ storage : \"CF\" })) Accessing the properties in the model In order to access the hstore properties directly on the model we could write our own custom accessor methods like so: 1 def storage 2 self . properties [ \"storage\" ] 3 end A neater approach is to use the store_accessor helper that creates these methods on the fly. This is how we can use it to create a storage method similar to the one above: 1 store_accessor :properties , :storage The result is the same in that we can access storage as an attribute directly on the Product instance. However, the store_accessor method gives us the ability to validate and set the property too! The Product class looks like this now: 1 class Product & lt ; ActiveRecord :: Base 2 store_accessor :properties , :storage 3 validates_presence_of :storage 4 end In the console, we can see that product behaves as expected: 1 p = Product . first 2 p . storage = nil 3 p . valid? # => false 4 p . errors . messages [ :storage ] # => [\"can't be blank\"] Storing different data types As mentioned, the hstore only stores keys / values as strings. However, since we can validate properties using standard Rails validations its possible to ensure a certain type is pushed to the hstore before it is saved in postgres as a string. That way we can always convert the type back when we want to use it. Here is an example where we might want to validate and store a number and then convert it back again when reading the value. Its possible to add this as a validation directly to the model, like so: 1 class Product & lt ; ActiveRecord :: Base 2 store_accessor :properties , :cost 3 validates_numericality_of :cost 4 end Once that is in place in the class, you can try out the functionality in the console as follows: 1 p . cost = \"this should not be allowed!\" 2 p . valid? 3 p . errors . messages [ :cost ] # => [\"is not a number\"] Storing nested hashes Since hstore stores keys and values as strings it means that any nested hashes will also be converted to a string. Here is an example. Given the following nested hash: 1 nested_properties_hash = { storage : \"5GB\" , cost : \"499\" , warranty : { first : \"OK\" , second : \"OK\" }} Setting this to the Product.properties is valid and so can be saved. However, note that the keys are converted to strings and that the nested part of the hash is escaped: 1 p . properties = nested_properties_hash 2 p . save 3 p . reload . properties # => {\"cost\"=>\"499\", \"storage\"=>\"5GB\", \"warranty\"=>\"{\\\"first\\\"=>\\\"OK\\\", \\\"second\\\"=>\\\"OK\\\"}\"} Notice that the top level is still a Hash but all the nested levels are now Strings (in our case 'warranty'). Its possible to convert all the nested levels to Hashes again using eval but I do not recommend this as your code can get really messy: 1 eval ( p . properties [ \"warranty\" ] ) # => {\"first\"=>\"OK\", \"second\"=>\"OK\"} Conclusion Using postgres hstore with Rails is really useful for flat (single level) key/value string store. Its possible to ensure a particular type by using validation and then convert that back after a read from the database. If there is a requirement to store more complex data structure, such as a nested Hash, then it would be better to use another storage mechanism such as a document base 'nosql' database. As always, here is an example application "},{"title":"Network Data Visualization graph using SigmaJS","url":"/2014/07/network-data-visualization-graph-using-sigmajs/","date":"2014-07-08 00:00:00 +0700","categories":["javascript client programming"],"body":"Sometimes its necessary to display data in something other than a table! Why? For example, when working with large datasets then it makes more sense to aggregate the data and display in the form of some type of chart like a pie chart or bar chart. In other cases the data might be geospatial data in which case a map would make an appropriate visualization. In todays post, I’ll demonstrate how its possible to render a Network data set using the SigmaJS library, in particular, using SigmaJS’s rather handy JSON parser. I’ll also demonstrate techniques for programmatically zooming into specific nodes by id using the SigmaJS Camera class. Rendering a basic network graph During this post I will work with a static JSON file that represents a small abstract network. The JSON file will be constructed so that it works out of the box with the SigmaJS JSON Parser class . In order for the JSON file to work with the SigmaJS JSON Parser it must contain two arrays ‘edges’ and ‘nodes’. Here is the sample JSON file I will work with. In order to convert his JSON file into a pretty Network visualization, we need to host the file somewhere. The easiest way is to use python HTTP server to act as a basic server for us: cd sigmajs_example python -m SimpleHTTPServer Now its possible to get the JSON file from the localhost server at port 8000. Like so: open http://localhost:8000/data/network.json After running the above open command you should see the raw JSON data being displayed in the browser. Obviously, some code is needed to manipulate this JSON file and turn it into our visualization. This can be done in JavaScript using SigmaJS! Firstly, a little HTML to get started to import the sigmajs files: <script src= \"../lib/sigma.min.js\" ></script> <script src= \"../lib/sigma.layout.forceAtlas2.min.js\" ></script> <script src= \"../lib/sigma.parsers.json.min.js\" ></script> Add a div tag as a placeholder for graph and give it some style so that we can see the graph output: <style> #network-graph { top : 0 ; bottom : 0 ; left : 0 ; right : 0 ; position : absolute ; } </style> <div id= \"network-graph\" ></div> Finally, the interesting part, where we use the Sigma JS JSON Parser. In this first example, I demonstrate the most basic usage of the parser which is to pass in the JSON file and the container id where Sigma will render the output: <script> sigma . parsers . json ( \"/data/network.json\" , { container : 'network-graph' }); </script> The output in the browser should be as follows: Layout the nodes in a circle Now lets layout the nodes in a circle. Firstly, it's important to know why the nodes in the previous example are being displayed in a diagonal line. It is due to the x and y values being set the way they are in the JSON file. For example Node 1 has x = 10, y = -10 which puts the node in the top right corner. For developers not used to drawing on the screen this is due to the y-axis being the reverse of what is usually expected say when drawing a graph. Usually the y-axis is positive going up. However, with screen drawing it is negative going up. Another thing to note is that the graph always draws to the extents (except when there is only one node in which case it is always rendered in the centre of the screen). So if there two nodes with the following nodes array in the JSON file: \"nodes\" : [ { \"id\" : \"1\" , \"label\" : \"Node 1\" , \"x\" : 0 , \"y\" : 0 }, { \"id\" : \"2\" , \"label\" : \"Node 2\" \"x\" : 1 , \"y\" : 1 } ] Then the output is two nodes diagonally opposite each other: As we add more nodes to the collection, at further and further x and y co-ordinates the diagram will spread out naturally. So back to the circlular representation of our network. If we want to do this we can borrow from the web. In this Github Issue thread there is a formula for rendering nodes as a circle: sigmaInstance . graph . nodes (). forEach ( function ( node , i , a ) { node . x = Math . cos ( Math . PI * 2 * i / a . length ); node . y = Math . sin ( Math . PI * 2 * i / a . length ); }); Where does the sigmaInstance variable come from? Well, back to our Sigma JSON Parser API, we can see there is room for a call back function which is passed, you guessed it, an instance of sigma! The full JS code looks like this (renamed sigmaInstance to 's'): sigma . parsers . json ( \"/data/network.json\" , { container : 'network-graph' }, function ( s ) { //This function is passed an instance of Sigma s //Initialize nodes as a circle s . graph . nodes (). forEach ( function ( node , i , a ) { node . x = Math . cos ( Math . PI * 2 * i / a . length ); node . y = Math . sin ( Math . PI * 2 * i / a . length ); }); //Call refresh to render the new graph s . refresh (); }); Note the call to refresh to essentially redraw the graph on the screen. The output should appear like so: Zoom into a specific node Let's say its necessary to pass a node id to a url parameter and have the graph render as normal, but afterwards zoom directly to that node on the graph. This could be useful for larger graph or for sharing links to specific nodes in the graph. The way to go about solving this in SigmaJS is to: Grab the selected node id from the url Find the node with that id by looking through the nodes collection Use the SigmaJS Camera class to zoom in to the selected node Here is the code to grab the selected node id from the url which I borrowed from one of these Stackoverflow answers : function getParameterByName ( name ) { var match = RegExp ( '[?&]' + name + '=([^&]*)' ). exec ( window . location . search ); return match && decodeURIComponent ( match [ 1 ]. replace ( /\\+/g , ' ' )); }; Since the passed in the parameter is node_id its possible to get that into a local variable as follows: nodeId = parseInt ( getParameterByName ( 'node_id' )); Next up is finding that node in the nodes array. Here is one solution: s . graph . nodes (). forEach ( function ( node , i , a ) { if ( node . id == nodeId ) { selectedNode = node ; return ; } }); Note its also possible to use the jQuery grep method for a more elegant approach to finding the node. Finally, we can zoom to that node using the SigmaJS Camera class! Here is one way how it is done. Adjust the zoom ratio to see the node closer or further away. Note that this code should be surrounded by an if statement to check if the selectedNode is not undefined to avoid a JavaScript error. s . cameras [ 0 ]. goTo ({ x : selectedNode [ 'read_cam0:x' ], y : selectedNode [ 'read_cam0:y' ], ratio : 0.1 }); Layout the nodes using the force! Finally, let's look at applying the force! All we need to do is call the startForceAtlas2 function on the Sigma graph instance like so: s . startForceAtlas2 (); In our example we want to call this after refresh so that the nodes are properly aligned in a circle before hand. The resulting output is as shown below: Conclusion Sigma.js is a powerful library for displaying network graphs. There are many powerful examples available on the web that demonstrate how to use this framework, for example datavisnetwork from moebio.com and French Senators Despite other, more common, data vizualisation tools supporting network graphs (for example d3js ) it is still worth using Sigma.js for network graphs especially if your application heavily depends on these type of vizualisations. Here is an example application which contains all the above examples in one small download! "}]}